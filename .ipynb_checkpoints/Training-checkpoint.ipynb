{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af429a14",
   "metadata": {},
   "source": [
    "# Importing different types of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0585ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a194e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.anaconda',\n",
       " '.conda',\n",
       " '.condarc',\n",
       " '.continuum',\n",
       " '.ipynb_checkpoints',\n",
       " '.ipython',\n",
       " '.jupyter',\n",
       " 'anaconda3',\n",
       " 'AppData',\n",
       " 'Application Data',\n",
       " 'Contacts',\n",
       " 'Cookies',\n",
       " 'Documents',\n",
       " 'Downloads',\n",
       " 'Favorites',\n",
       " 'Links',\n",
       " 'Local Settings',\n",
       " 'Music',\n",
       " 'My Documents',\n",
       " 'NetHood',\n",
       " 'NTUSER.DAT',\n",
       " 'ntuser.dat.LOG1',\n",
       " 'ntuser.dat.LOG2',\n",
       " 'NTUSER.DAT{a2332f18-cdbf-11ec-8680-002248483d79}.TM.blf',\n",
       " 'NTUSER.DAT{a2332f18-cdbf-11ec-8680-002248483d79}.TMContainer00000000000000000001.regtrans-ms',\n",
       " 'NTUSER.DAT{a2332f18-cdbf-11ec-8680-002248483d79}.TMContainer00000000000000000002.regtrans-ms',\n",
       " 'ntuser.ini',\n",
       " 'OneDrive',\n",
       " 'PrintHood',\n",
       " 'Recent',\n",
       " 'Saved Games',\n",
       " 'Searches',\n",
       " 'SendTo',\n",
       " 'Start Menu',\n",
       " 'Templates',\n",
       " 'Training.ipynb',\n",
       " 'Videos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dc4c7",
   "metadata": {},
   "source": [
    "A delimiter is one or more characters that separate text strings. Common delimiters are commas (,), semicolon (;), quotes ( \", ' ), braces ( {}), pipes (|), or slashes ( / \\ ). When a program stores sequential or tabular data, it delimits each item of data with a predefined character. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdce08f",
   "metadata": {},
   "source": [
    "### 1.1 Going under the Hood of pandas read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692041ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d5dde",
   "metadata": {},
   "source": [
    "### 1.1.2 Types of files one can import from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c879b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read_clipboard',\n",
       " 'read_csv',\n",
       " 'read_excel',\n",
       " 'read_feather',\n",
       " 'read_fwf',\n",
       " 'read_gbq',\n",
       " 'read_hdf',\n",
       " 'read_html',\n",
       " 'read_json',\n",
       " 'read_orc',\n",
       " 'read_parquet',\n",
       " 'read_pickle',\n",
       " 'read_sas',\n",
       " 'read_spss',\n",
       " 'read_sql',\n",
       " 'read_sql_query',\n",
       " 'read_sql_table',\n",
       " 'read_stata',\n",
       " 'read_table',\n",
       " 'read_xml']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile(r'read')\n",
    "list(filter(regex.match, dir(pd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3310d20",
   "metadata": {},
   "source": [
    "The clipboard is a temporary storage area in your computerâ€™s memory that stores the information you copy or cut. The information can be text, images, or other types of data. You can then paste the contents of the clipboard into another location, such as a document or an email "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20a0fc",
   "metadata": {},
   "source": [
    "## CSV vs. Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7a91e",
   "metadata": {},
   "source": [
    "CSV stands for Comma-Separated Values, while Excel is a spreadsheet application that saves files into its own format 123. CSV files are used for storing data in tabular format and are just plain text files with values separated by commas. They can be opened with text editors (such as Notepad) and are faster to process and open. However, they cannot store other information like formatting, links, charts, pictures, etc13 On the other hand, Excel files are binary files with multiple worksheets that can store formatting and perform operations on data. They can contain symbols, links, charts, pictures, etc., and are harder to read with larger sets of data13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9686ae80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Position</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>HR</td>\n",
       "      <td>Manager</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Bob Johnson</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Coordinator</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Charlie Wilson</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee ID            Name   Department              Position  Salary\n",
       "0          101      John Smith           HR               Manager   75000\n",
       "1          102        Jane Doe      Finance               Analyst   60000\n",
       "2          103     Bob Johnson  Engineering              Engineer   80000\n",
       "3          104     Alice Brown    Marketing           Coordinator   50000\n",
       "4          105  Charlie Wilson        Sales  Sales Representative   65000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Employee ID': [101, 102, 103, 104, 105],\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Alice Brown', 'Charlie Wilson'],\n",
    "    'Department': ['HR', 'Finance', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'Position': ['Manager', 'Analyst', 'Engineer', 'Coordinator', 'Sales Representative'],\n",
    "    'Salary': [75000, 60000, 80000, 50000, 65000]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "employee_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c7f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.to_csv('Test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21520fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.to_excel('Test2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0336ae",
   "metadata": {},
   "source": [
    "## Feather files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0306f7",
   "metadata": {},
   "source": [
    "A Feather file is a binary file format for efficiently storing and sharing data between different programming languages and data analysis tools. Feather was designed to be lightweight and to optimize data transfer between languages, particularly for data analysis libraries like pandas in Python and Apache Arrow in other languages like R, Julia, and more.\n",
    "\n",
    "Feather files have a few key features:\n",
    "\n",
    "Language-Agnostic: Feather files are designed to be language-agnostic, which means you can read and write them from multiple programming languages without losing data integrity or performance.\n",
    "\n",
    "Columnar Storage: Feather stores data in a columnar format, which is often more efficient for data analysis and allows for faster read and write operations, especially when working with large datasets.\n",
    "\n",
    "Metadata: Feather files include metadata that helps describe the data, such as data types and column names, making it self-descriptive.\n",
    "\n",
    "Efficient Serialization: Feather is optimized for fast serialization and deserialization, making it suitable for reading and writing data frames or tables quickly.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def55a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Column1 Column2\n",
      "0        1       A\n",
      "1        2       B\n",
      "2        3       C\n",
      "3        4       D\n",
      "4        5       E\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Column1': [1, 2, 3, 4, 5], 'Column2': ['A', 'B', 'C', 'D', 'E']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the DataFrame to a Feather file\n",
    "df.to_feather('example.feather')\n",
    "\n",
    "# Read the Feather file back into a DataFrame\n",
    "df_from_feather = pd.read_feather('example.feather')\n",
    "\n",
    "# Display the DataFrame read from Feather\n",
    "print(df_from_feather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80f44c",
   "metadata": {},
   "source": [
    "## Fixed width Files fwf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d7d29",
   "metadata": {},
   "source": [
    "The `read_fwf` function in pandas is used when you have data in a fixed-width format, and you want to read that data into a DataFrame for further analysis and manipulation. Fixed-width format data is a type of plain text data where each column has a specified width, and the data within each column is aligned to those widths.\n",
    "\n",
    "Here are some common scenarios when you might use `read_fwf`:\n",
    "\n",
    "1. **Legacy Data Formats**: Fixed-width files were common in legacy systems or data sources where data was stored and exchanged in a format where each column's position was predetermined. If you need to work with such data, you would use `read_fwf` to parse and load it into a DataFrame.\n",
    "\n",
    "2. **Government Data**: Government agencies and organizations often provide data in fixed-width format files. Examples include census data, economic indicators, or demographic data.\n",
    "\n",
    "3. **Mainframe Systems**: Data exported from mainframe systems or older databases may be in fixed-width format. Reading this data with `read_fwf` can be essential for data analysis or migration to modern systems.\n",
    "\n",
    "4. **Financial Data**: Financial data, including stock market data or financial reports, may be distributed in fixed-width format. Analysts often use `read_fwf` to load this data for analysis.\n",
    "\n",
    "5. **Custom Data Export Formats**: Sometimes, organizations or systems export data in custom fixed-width formats for specific applications. If you encounter such data, `read_fwf` can help you parse and work with it.\n",
    "\n",
    "When working with fixed-width data, it's crucial to know the exact column widths and data types in the file, as these details are necessary to correctly parse the data. You'll typically need to provide the `colspecs` parameter to specify the start and end positions of each column.\n",
    "\n",
    "Here's a general use case for `read_fwf`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Define the column widths for your fixed-width file\n",
    "colspecs = [(0, 5), (5, 10), (10, 15)]  # Adjust to match your data\n",
    "\n",
    "# Read the FWF file into a DataFrame\n",
    "df = pd.read_fwf('data.fwf', colspecs=colspecs)\n",
    "\n",
    "# Perform data analysis and manipulation using the DataFrame\n",
    "```\n",
    "\n",
    "In this example, `colspecs` specifies the start and end positions of three columns in the FWF file, and `read_fwf` reads the data accordingly. You should adjust `colspecs` to match the specific formatting of your FWF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f639bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\crist\\Downloads\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad38bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0\n",
      "0  123456789\n",
      "1  987654321\n",
      "2  456789123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_fwf(filepath, colspecs='infer', header=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780919e1",
   "metadata": {},
   "source": [
    "## Benefits of FWF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19decddb",
   "metadata": {},
   "source": [
    "Fixed-width format (FWF) files have several benefits in data storage and processing:\n",
    "\n",
    "Predictable Structure: FWF files have a fixed structure where each field occupies a specific number of characters or positions within each record. This predictability makes it easy to parse and read the data accurately.\n",
    "\n",
    "Human-Readable: FWF files are often human-readable because of their fixed-column layout. This makes it easier for people to inspect the data without the need for specialized software.\n",
    "\n",
    "Efficiency: FWF files can be more memory-efficient and faster to read and write compared to variable-width files (e.g., CSV) because there's no need for delimiters. Processing fixed-width data can be faster, especially with large datasets.\n",
    "\n",
    "Preservation of Leading Zeros: FWF files are useful for storing data where leading zeros are significant (e.g., ZIP codes, product codes, or identification numbers) because they maintain the exact character positions.\n",
    "\n",
    "Data Integrity: FWF files are less prone to data corruption due to missing or misplaced delimiters that can occur in variable-width files.\n",
    "\n",
    "Compatibility: FWF files are well-suited for integration with legacy systems or other software that expects data in a fixed-width format.\n",
    "\n",
    "Data Validation: The fixed-width format makes it easier to enforce data validation rules as data must conform to the specified column widths.\n",
    "\n",
    "Alignment: When displaying FWF data in a text editor or fixed-width font, the columns align neatly, making it easier for users to visually interpret the data.\n",
    "\n",
    "However, it's essential to consider the specific use case and requirements when choosing between FWF and other data storage formats like CSV, TSV, or JSON. FWF is most beneficial when data has a consistent and predictable structure. If your data has varying column widths or complex structures, other formats may be more suitable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f60f49",
   "metadata": {},
   "source": [
    "## Google Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your BigQuery SQL query as a string\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  column1,\n",
    "  column2\n",
    "FROM\n",
    "  your_project_id.your_dataset.your_table\n",
    "WHERE\n",
    "  some_condition\n",
    "\"\"\"\n",
    "\n",
    "# Set up the BigQuery authentication (you need to authenticate to access your BigQuery data)\n",
    "# You can use your Google Cloud credentials JSON file or application default credentials.\n",
    "# For application default credentials, you can use:\n",
    "# pd.read_gbq(query, project_id=your_project_id, dialect='standard')\n",
    "\n",
    "# Authenticate using your Google Cloud credentials JSON file\n",
    "# pd.read_gbq(query, project_id=your_project_id, private_key='path/to/your/credentials.json', dialect='standard')\n",
    "\n",
    "# Use the read_gbq function to execute the query and retrieve the data into a DataFrame\n",
    "df = pd.read_gbq(query, project_id=your_project_id, dialect='standard')\n",
    "\n",
    "# Now, you can work with the data in the DataFrame 'df'\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c1ca2",
   "metadata": {},
   "source": [
    "## Benefits of Google Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd9c07",
   "metadata": {},
   "source": [
    "Google BigQuery is a fully-managed, serverless, and highly scalable data warehouse and analytics platform offered by Google Cloud. It provides several benefits for organizations and data professionals:\n",
    "\n",
    "Scalability: BigQuery is designed to handle massive datasets with ease. It can automatically scale to accommodate growing data volumes, ensuring that you can run complex queries on large datasets without worrying about infrastructure limitations.\n",
    "\n",
    "Serverless: You don't need to provision or manage servers when using BigQuery. It's a serverless platform, which means Google takes care of infrastructure management, including hardware and software updates.\n",
    "\n",
    "Speed: BigQuery is known for its blazing-fast query performance. It uses a distributed architecture and columnar storage to execute queries quickly, even on petabyte-scale datasets.\n",
    "\n",
    "Cost-Effective: With BigQuery's pay-as-you-go pricing model, you only pay for the data you query and store. It eliminates the need for upfront capital expenditures and allows you to control costs effectively.\n",
    "\n",
    "SQL Support: BigQuery supports standard SQL, making it easy for data analysts and SQL developers to write and run queries without the need to learn a new query language.\n",
    "\n",
    "Integration: It integrates seamlessly with other Google Cloud services, such as Google Cloud Storage, Google Data Studio, and Google Sheets, allowing you to build end-to-end data pipelines and analytics solutions.\n",
    "\n",
    "Data Warehousing and Data Lake Capabilities: BigQuery can function as both a data warehouse and a data lake. You can store structured and semi-structured data in BigQuery tables or query data directly from external storage like Google Cloud Storage.\n",
    "\n",
    "Security: Google Cloud provides robust security features, including encryption at rest and in transit, identity and access management (IAM), and audit logs. It complies with industry standards and certifications.\n",
    "\n",
    "Real-Time Data Analysis: BigQuery supports real-time data streaming, enabling you to analyze data as it arrives, making it suitable for real-time analytics use cases.\n",
    "\n",
    "Machine Learning Integration: You can leverage Google's machine learning capabilities and services like BigQuery ML to build and deploy machine learning models directly within BigQuery.\n",
    "\n",
    "Geo-spatial and Advanced Analytics: BigQuery offers support for geospatial data and a wide range of advanced analytics functions, including window functions, machine learning, and statistical analysis.\n",
    "\n",
    "Data Sharing and Collaboration: You can easily share datasets and queries with others in your organization or externally, facilitating collaboration and data sharing.\n",
    "\n",
    "Automatic Backups and High Availability: BigQuery automatically takes care of data backups and provides high availability, ensuring that your data is safe and accessible.\n",
    "\n",
    "Cost Optimization Tools: Google Cloud provides cost optimization tools and features to help you analyze and control your BigQuery costs, making it easier to manage your budget.\n",
    "\n",
    "Community and Support: BigQuery has a large and active user community, and Google Cloud offers various levels of support, including documentation, forums, and premium support options.\n",
    "\n",
    "Overall, Google BigQuery is a powerful and versatile platform that can help organizations make data-driven decisions, gain insights from their data, and leverage the benefits of cloud computing without the complexity of managing infrastructure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40f53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f9073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
