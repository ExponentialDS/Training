{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af429a14",
   "metadata": {},
   "source": [
    "# Importing different types of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dc4c7",
   "metadata": {},
   "source": [
    "A delimiter is one or more characters that separate text strings. Common delimiters are commas (,), semicolon (;), quotes ( \", ' ), braces ( {}), pipes (|), or slashes ( / \\ ). When a program stores sequential or tabular data, it delimits each item of data with a predefined character. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdce08f",
   "metadata": {},
   "source": [
    "### 1.1 Going under the Hood of pandas read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692041ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d5dde",
   "metadata": {},
   "source": [
    "### 1.1.2 Types of files one can import from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c879b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r'read')\n",
    "list(filter(regex.match, dir(pd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3310d20",
   "metadata": {},
   "source": [
    "The clipboard is a temporary storage area in your computerâ€™s memory that stores the information you copy or cut. The information can be text, images, or other types of data. You can then paste the contents of the clipboard into another location, such as a document or an email "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20a0fc",
   "metadata": {},
   "source": [
    "## CSV vs. Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7a91e",
   "metadata": {},
   "source": [
    "CSV stands for Comma-Separated Values, while Excel is a spreadsheet application that saves files into its own format 123. CSV files are used for storing data in tabular format and are just plain text files with values separated by commas. They can be opened with text editors (such as Notepad) and are faster to process and open. However, they cannot store other information like formatting, links, charts, pictures, etc13 On the other hand, Excel files are binary files with multiple worksheets that can store formatting and perform operations on data. They can contain symbols, links, charts, pictures, etc., and are harder to read with larger sets of data13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Employee ID': [101, 102, 103, 104, 105],\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Alice Brown', 'Charlie Wilson'],\n",
    "    'Department': ['HR', 'Finance', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'Position': ['Manager', 'Analyst', 'Engineer', 'Coordinator', 'Sales Representative'],\n",
    "    'Salary': [75000, 60000, 80000, 50000, 65000]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "employee_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.to_csv('Test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21520fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.to_excel('Test2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0336ae",
   "metadata": {},
   "source": [
    "## Feather files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0306f7",
   "metadata": {},
   "source": [
    "A Feather file is a binary file format for efficiently storing and sharing data between different programming languages and data analysis tools. Feather was designed to be lightweight and to optimize data transfer between languages, particularly for data analysis libraries like pandas in Python and Apache Arrow in other languages like R, Julia, and more.\n",
    "\n",
    "Feather files have a few key features:\n",
    "\n",
    "Language-Agnostic: Feather files are designed to be language-agnostic, which means you can read and write them from multiple programming languages without losing data integrity or performance.\n",
    "\n",
    "Columnar Storage: Feather stores data in a columnar format, which is often more efficient for data analysis and allows for faster read and write operations, especially when working with large datasets.\n",
    "\n",
    "Metadata: Feather files include metadata that helps describe the data, such as data types and column names, making it self-descriptive.\n",
    "\n",
    "Efficient Serialization: Feather is optimized for fast serialization and deserialization, making it suitable for reading and writing data frames or tables quickly.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def55a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Column1': [1, 2, 3, 4, 5], 'Column2': ['A', 'B', 'C', 'D', 'E']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the DataFrame to a Feather file\n",
    "df.to_feather('example.feather')\n",
    "\n",
    "# Read the Feather file back into a DataFrame\n",
    "df_from_feather = pd.read_feather('example.feather')\n",
    "\n",
    "# Display the DataFrame read from Feather\n",
    "print(df_from_feather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80f44c",
   "metadata": {},
   "source": [
    "## Fixed width Files fwf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d7d29",
   "metadata": {},
   "source": [
    "The `read_fwf` function in pandas is used when you have data in a fixed-width format, and you want to read that data into a DataFrame for further analysis and manipulation. Fixed-width format data is a type of plain text data where each column has a specified width, and the data within each column is aligned to those widths.\n",
    "\n",
    "Here are some common scenarios when you might use `read_fwf`:\n",
    "\n",
    "1. **Legacy Data Formats**: Fixed-width files were common in legacy systems or data sources where data was stored and exchanged in a format where each column's position was predetermined. If you need to work with such data, you would use `read_fwf` to parse and load it into a DataFrame.\n",
    "\n",
    "2. **Government Data**: Government agencies and organizations often provide data in fixed-width format files. Examples include census data, economic indicators, or demographic data.\n",
    "\n",
    "3. **Mainframe Systems**: Data exported from mainframe systems or older databases may be in fixed-width format. Reading this data with `read_fwf` can be essential for data analysis or migration to modern systems.\n",
    "\n",
    "4. **Financial Data**: Financial data, including stock market data or financial reports, may be distributed in fixed-width format. Analysts often use `read_fwf` to load this data for analysis.\n",
    "\n",
    "5. **Custom Data Export Formats**: Sometimes, organizations or systems export data in custom fixed-width formats for specific applications. If you encounter such data, `read_fwf` can help you parse and work with it.\n",
    "\n",
    "When working with fixed-width data, it's crucial to know the exact column widths and data types in the file, as these details are necessary to correctly parse the data. You'll typically need to provide the `colspecs` parameter to specify the start and end positions of each column.\n",
    "\n",
    "Here's a general use case for `read_fwf`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Define the column widths for your fixed-width file\n",
    "colspecs = [(0, 5), (5, 10), (10, 15)]  # Adjust to match your data\n",
    "\n",
    "# Read the FWF file into a DataFrame\n",
    "df = pd.read_fwf('data.fwf', colspecs=colspecs)\n",
    "\n",
    "# Perform data analysis and manipulation using the DataFrame\n",
    "```\n",
    "\n",
    "In this example, `colspecs` specifies the start and end positions of three columns in the FWF file, and `read_fwf` reads the data accordingly. You should adjust `colspecs` to match the specific formatting of your FWF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\crist\\Downloads\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_fwf(filepath, colspecs='infer', header=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780919e1",
   "metadata": {},
   "source": [
    "## Benefits of FWF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19decddb",
   "metadata": {},
   "source": [
    "Fixed-width format (FWF) files have several benefits in data storage and processing:\n",
    "\n",
    "Predictable Structure: FWF files have a fixed structure where each field occupies a specific number of characters or positions within each record. This predictability makes it easy to parse and read the data accurately.\n",
    "\n",
    "Human-Readable: FWF files are often human-readable because of their fixed-column layout. This makes it easier for people to inspect the data without the need for specialized software.\n",
    "\n",
    "Efficiency: FWF files can be more memory-efficient and faster to read and write compared to variable-width files (e.g., CSV) because there's no need for delimiters. Processing fixed-width data can be faster, especially with large datasets.\n",
    "\n",
    "Preservation of Leading Zeros: FWF files are useful for storing data where leading zeros are significant (e.g., ZIP codes, product codes, or identification numbers) because they maintain the exact character positions.\n",
    "\n",
    "Data Integrity: FWF files are less prone to data corruption due to missing or misplaced delimiters that can occur in variable-width files.\n",
    "\n",
    "Compatibility: FWF files are well-suited for integration with legacy systems or other software that expects data in a fixed-width format.\n",
    "\n",
    "Data Validation: The fixed-width format makes it easier to enforce data validation rules as data must conform to the specified column widths.\n",
    "\n",
    "Alignment: When displaying FWF data in a text editor or fixed-width font, the columns align neatly, making it easier for users to visually interpret the data.\n",
    "\n",
    "However, it's essential to consider the specific use case and requirements when choosing between FWF and other data storage formats like CSV, TSV, or JSON. FWF is most beneficial when data has a consistent and predictable structure. If your data has varying column widths or complex structures, other formats may be more suitable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f60f49",
   "metadata": {},
   "source": [
    "## Google Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your BigQuery SQL query as a string\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  column1,\n",
    "  column2\n",
    "FROM\n",
    "  your_project_id.your_dataset.your_table\n",
    "WHERE\n",
    "  some_condition\n",
    "\"\"\"\n",
    "\n",
    "# Set up the BigQuery authentication (you need to authenticate to access your BigQuery data)\n",
    "# You can use your Google Cloud credentials JSON file or application default credentials.\n",
    "# For application default credentials, you can use:\n",
    "# pd.read_gbq(query, project_id=your_project_id, dialect='standard')\n",
    "\n",
    "# Authenticate using your Google Cloud credentials JSON file\n",
    "# pd.read_gbq(query, project_id=your_project_id, private_key='path/to/your/credentials.json', dialect='standard')\n",
    "\n",
    "# Use the read_gbq function to execute the query and retrieve the data into a DataFrame\n",
    "df = pd.read_gbq(query, project_id=your_project_id, dialect='standard')\n",
    "\n",
    "# Now, you can work with the data in the DataFrame 'df'\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c1ca2",
   "metadata": {},
   "source": [
    "## Benefits of Google Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd9c07",
   "metadata": {},
   "source": [
    "Google BigQuery is a fully-managed, serverless, and highly scalable data warehouse and analytics platform offered by Google Cloud. It provides several benefits for organizations and data professionals:\n",
    "\n",
    "Scalability: BigQuery is designed to handle massive datasets with ease. It can automatically scale to accommodate growing data volumes, ensuring that you can run complex queries on large datasets without worrying about infrastructure limitations.\n",
    "\n",
    "Serverless: You don't need to provision or manage servers when using BigQuery. It's a serverless platform, which means Google takes care of infrastructure management, including hardware and software updates.\n",
    "\n",
    "Speed: BigQuery is known for its blazing-fast query performance. It uses a distributed architecture and columnar storage to execute queries quickly, even on petabyte-scale datasets.\n",
    "\n",
    "Cost-Effective: With BigQuery's pay-as-you-go pricing model, you only pay for the data you query and store. It eliminates the need for upfront capital expenditures and allows you to control costs effectively.\n",
    "\n",
    "SQL Support: BigQuery supports standard SQL, making it easy for data analysts and SQL developers to write and run queries without the need to learn a new query language.\n",
    "\n",
    "Integration: It integrates seamlessly with other Google Cloud services, such as Google Cloud Storage, Google Data Studio, and Google Sheets, allowing you to build end-to-end data pipelines and analytics solutions.\n",
    "\n",
    "Data Warehousing and Data Lake Capabilities: BigQuery can function as both a data warehouse and a data lake. You can store structured and semi-structured data in BigQuery tables or query data directly from external storage like Google Cloud Storage.\n",
    "\n",
    "Security: Google Cloud provides robust security features, including encryption at rest and in transit, identity and access management (IAM), and audit logs. It complies with industry standards and certifications.\n",
    "\n",
    "Real-Time Data Analysis: BigQuery supports real-time data streaming, enabling you to analyze data as it arrives, making it suitable for real-time analytics use cases.\n",
    "\n",
    "Machine Learning Integration: You can leverage Google's machine learning capabilities and services like BigQuery ML to build and deploy machine learning models directly within BigQuery.\n",
    "\n",
    "Geo-spatial and Advanced Analytics: BigQuery offers support for geospatial data and a wide range of advanced analytics functions, including window functions, machine learning, and statistical analysis.\n",
    "\n",
    "Data Sharing and Collaboration: You can easily share datasets and queries with others in your organization or externally, facilitating collaboration and data sharing.\n",
    "\n",
    "Automatic Backups and High Availability: BigQuery automatically takes care of data backups and provides high availability, ensuring that your data is safe and accessible.\n",
    "\n",
    "Cost Optimization Tools: Google Cloud provides cost optimization tools and features to help you analyze and control your BigQuery costs, making it easier to manage your budget.\n",
    "\n",
    "Community and Support: BigQuery has a large and active user community, and Google Cloud offers various levels of support, including documentation, forums, and premium support options.\n",
    "\n",
    "Overall, Google BigQuery is a powerful and versatile platform that can help organizations make data-driven decisions, gain insights from their data, and leverage the benefits of cloud computing without the complexity of managing infrastructure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7b952",
   "metadata": {},
   "source": [
    "## HDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530bfe9",
   "metadata": {},
   "source": [
    "\n",
    "An HDF file, which stands for \"Hierarchical Data Format\" file, is a file format designed for storing and organizing large amounts of data. HDF files are particularly popular in the scientific and engineering communities for applications that involve complex and multidimensional datasets. Here are some key features of HDF files:\n",
    "\n",
    "Hierarchical Structure: HDF files have a hierarchical structure, similar to a file system, where data can be organized into groups and datasets. This hierarchical organization makes it easy to store and manage structured data.\n",
    "\n",
    "Support for Various Data Types: HDF supports a wide range of data types, including numerical data (integers, floats, etc.), text, images, and more. This versatility makes it suitable for a broad spectrum of applications.\n",
    "\n",
    "Compression: HDF files can be compressed, which helps reduce file size while preserving data integrity. This is particularly useful when dealing with large datasets.\n",
    "\n",
    "Portability: HDF files are designed to be platform-independent. You can create and access HDF files on various operating systems, including Windows, macOS, and Linux.\n",
    "\n",
    "Data Chunking: HDF allows data to be divided into smaller, regularly sized chunks. This can improve data access and manipulation, especially for large datasets.\n",
    "\n",
    "Metadata: You can attach metadata to HDF datasets, providing additional information about the data's content, source, and any relevant attributes.\n",
    "\n",
    "Libraries and APIs: There are libraries and APIs available for various programming languages (e.g., HDF5 for C/C++, h5py for Python) that allow developers to work with HDF files, making it easier to create, read, write, and manipulate data stored in HDF format.\n",
    "\n",
    "HDF files are commonly used in fields such as astronomy, geoscience, climate modeling, bioinformatics, and more, where researchers need to store and analyze large and complex datasets. The two major versions of HDF are HDF4 and HDF5, with HDF5 being the more modern and widely adopted version due to its improved capabilities and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209f562",
   "metadata": {},
   "source": [
    "## Create dummy HDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Define your dataset (a simple 2D array in this example)\n",
    "data = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create an HDF5 file in write mode\n",
    "with h5py.File('my_dataset.h5', 'w') as hdf_file:\n",
    "    # Create a dataset and write data to it\n",
    "    dataset = hdf_file.create_dataset('my_data', data=data)\n",
    "\n",
    "    # Optionally, add attributes and metadata to the dataset\n",
    "    dataset.attrs['description'] = 'My sample dataset'\n",
    "    dataset.attrs['author'] = 'Your Name'\n",
    "\n",
    "# The HDF5 file is automatically closed when the 'with' block exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f4167",
   "metadata": {},
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b08637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('my_dataset.h5', 'r') as hdf_file:\n",
    "    # Access the dataset\n",
    "    dataset = hdf_file['my_data']\n",
    "    \n",
    "    # Convert the dataset to a NumPy array\n",
    "    data = dataset[()]\n",
    "    \n",
    "    # Print the contents of the array\n",
    "    print(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dd231",
   "metadata": {},
   "source": [
    "## Convert HDF File to Dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('my_dataset.h5', 'r') as hdf_file:\n",
    "    # Access the dataset\n",
    "    dataset = hdf_file['my_data']\n",
    "    \n",
    "    # Convert the dataset to a pandas DataFrame\n",
    "    df = pd.DataFrame(dataset[()])\n",
    "\n",
    "# Now you have the data in a pandas DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831b4d9",
   "metadata": {},
   "source": [
    "## HTML Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some example data\n",
    "data = [\n",
    "    {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
    "    {\"name\": \"Bob\", \"age\": 35, \"city\": \"Chicago\"},\n",
    "]\n",
    "\n",
    "# Create an HTML string\n",
    "html_string = \"<html>\\n\"\n",
    "html_string += \"<head><title>Sample Dataset</title></head>\\n\"\n",
    "html_string += \"<body>\\n\"\n",
    "html_string += \"<h1>Sample Dataset</h1>\\n\"\n",
    "html_string += \"<table border='1'>\\n\"\n",
    "html_string += \"<tr><th>Name</th><th>Age</th><th>City</th></tr>\\n\"\n",
    "\n",
    "for record in data:\n",
    "    html_string += \"<tr>\"\n",
    "    html_string += f\"<td>{record['name']}</td>\"\n",
    "    html_string += f\"<td>{record['age']}</td>\"\n",
    "    html_string += f\"<td>{record['city']}</td>\"\n",
    "    html_string += \"</tr>\\n\"\n",
    "\n",
    "html_string += \"</table>\\n\"\n",
    "html_string += \"</body>\\n\"\n",
    "html_string += \"</html>\"\n",
    "\n",
    "# Save the HTML string to a file\n",
    "with open(\"sample_dataset.html\", \"w\") as html_file:\n",
    "    html_file.write(html_string)\n",
    "\n",
    "print(\"Sample dataset HTML file created: sample_dataset.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7803ade",
   "metadata": {},
   "source": [
    "## When to parse HTML files in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1878762d",
   "metadata": {},
   "source": [
    "You would parse HTML files in Python when you need to extract and manipulate data or information from HTML documents. Parsing HTML is common in various scenarios, including:\n",
    "\n",
    "Web Scraping: When you want to extract data from websites for purposes such as data analysis, research, or building web applications. Python libraries like Beautiful Soup and Scrapy are often used for web scraping tasks.\n",
    "\n",
    "Data Extraction: When you need to extract structured data from HTML documents, such as tables, lists, or specific elements, for further processing or analysis.\n",
    "\n",
    "Web Testing and Automation: When you automate web interactions or perform web testing, you may need to parse HTML to locate and interact with specific elements on a web page.\n",
    "\n",
    "Data Cleaning: When you have HTML-encoded content in your dataset and you want to convert it to plain text or extract specific information.\n",
    "\n",
    "Generating Dynamic Content: When you dynamically generate HTML documents based on data from databases or other sources.\n",
    "\n",
    "Web Development: When building web applications using frameworks like Flask or Django, you often work with HTML templates to render dynamic content.\n",
    "\n",
    "Python offers several libraries and tools for parsing HTML, including:\n",
    "\n",
    "Beautiful Soup: A popular library for parsing HTML and XML documents, making it easy to navigate and search the document's structure.\n",
    "\n",
    "lxml: A high-performance library for parsing XML and HTML documents. It's often used in combination with XPath for advanced data extraction.\n",
    "\n",
    "html.parser: The built-in HTML parser in Python's standard library, which can be used for basic HTML parsing tasks.\n",
    "\n",
    "Selenium: A tool often used for web testing and automation, allowing you to programmatically interact with web pages and extract data.\n",
    "\n",
    "Scrapy: A powerful web crawling and scraping framework that provides a comprehensive set of tools for extracting and processing data from websites.\n",
    "\n",
    "The specific use case for parsing HTML in Python depends on your project requirements. Whether it's extracting data from a website, cleaning and transforming HTML-encoded content, or interacting with web pages programmatically, Python offers a range of tools and libraries to help you achieve your goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a1d3b",
   "metadata": {},
   "source": [
    "## JSON "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a4f32",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) files and DataFrames are both data structures used for representing and storing data, but they have different characteristics and purposes. Here are the key differences between them:\n",
    "\n",
    "Data Representation:\n",
    "\n",
    "JSON: JSON is a lightweight, text-based data interchange format. It's designed to represent structured data as a collection of key-value pairs, where keys are strings, and values can be strings, numbers, objects, arrays, booleans, or null. JSON is often used for data exchange between systems and for configuration files.\n",
    "\n",
    "DataFrame: A DataFrame is a tabular data structure commonly used in data analysis and manipulation. It's a two-dimensional, labeled data structure with rows and columns, similar to a spreadsheet or SQL table. Each column can have a different data type, making it suitable for heterogeneous data.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "JSON: JSON is typically used for data exchange between applications, configuration files, or representing data in a semi-structured format. It's commonly used in web APIs, configuration files, and data serialization.\n",
    "\n",
    "DataFrame: DataFrames are used for data analysis, data manipulation, and exploration tasks. They are a fundamental data structure in data science libraries like pandas in Python and are used for tasks such as filtering, grouping, aggregation, and visualization.\n",
    "\n",
    "Storage Format:\n",
    "\n",
    "JSON: JSON is stored as plain text and is human-readable. It's a flexible format for representing structured data, but it may not be as space-efficient as binary formats for large datasets.\n",
    "\n",
    "DataFrame: DataFrames are typically stored in memory, and they can be serialized to various formats like CSV, Excel, HDF5, or Parquet for storage. These formats may offer compression and better storage efficiency compared to JSON.\n",
    "\n",
    "Schema and Type Information:\n",
    "\n",
    "JSON: JSON does not have a predefined schema or type information. It's up to the application to interpret and validate the data.\n",
    "\n",
    "DataFrame: DataFrames have a schema that defines the data types and column names. This schema enforces type consistency within columns, making it easier to work with structured data.\n",
    "\n",
    "Access and Manipulation:\n",
    "\n",
    "JSON: Accessing and manipulating data in JSON typically involves parsing the text and working with the resulting data structure in your programming language. Libraries like json in Python are commonly used for this purpose.\n",
    "\n",
    "DataFrame: DataFrames provide a high-level API for data manipulation and analysis. Libraries like pandas offer powerful functions for filtering, transforming, aggregating, and visualizing data in a tabular format.\n",
    "\n",
    "In summary, JSON files are more suited for data interchange and configuration, while DataFrames are designed for data analysis and manipulation. The choice between the two depends on your specific use case and the type of data you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e4775",
   "metadata": {},
   "source": [
    "## Create a dummy JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a dictionary with sample data\n",
    "data = {\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"name\": \"John\",\n",
    "            \"age\": 30,\n",
    "            \"department\": \"HR\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Alice\",\n",
    "            \"age\": 28,\n",
    "            \"department\": \"Engineering\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Bob\",\n",
    "            \"age\": 35,\n",
    "            \"department\": \"Marketing\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Serialize the data to a JSON-formatted string\n",
    "json_string = json.dumps(data, indent=4)\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_string)\n",
    "\n",
    "# Alternatively, save the data to a JSON file\n",
    "with open(\"sample_dataset.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "print(\"Sample JSON dataset created: sample_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e463d",
   "metadata": {},
   "source": [
    "## Convert JSON to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1645867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the JSON data (you can also read it from a file)\n",
    "data = {\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"name\": \"John\",\n",
    "            \"age\": 30,\n",
    "            \"department\": \"HR\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Alice\",\n",
    "            \"age\": 28,\n",
    "            \"department\": \"Engineering\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Bob\",\n",
    "            \"age\": 35,\n",
    "            \"department\": \"Marketing\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = pd.DataFrame(data[\"employees\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0784e96",
   "metadata": {},
   "source": [
    "## ORC Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80675a",
   "metadata": {},
   "source": [
    "An ORC (Optimized Row Columnar) file is a columnar storage file format used for storing and managing large volumes of structured data efficiently. It was developed as an open-source project by the Hadoop ecosystem and is widely used in big data processing frameworks like Apache Hive, Apache Spark, and Apache Impala.\n",
    "\n",
    "Key characteristics and features of ORC files include:\n",
    "\n",
    "Columnar Storage: Unlike traditional row-based storage formats, ORC files store data column by column. This allows for better compression and encoding of data because similar data types within a column can be stored together.\n",
    "\n",
    "Compression: ORC files employ various compression techniques to reduce the storage space required. They often use lightweight compression algorithms like Zlib, Snappy, or LZO.\n",
    "\n",
    "Predicate Pushdown: ORC files support predicate pushdown, which means that query engines can apply filtering and predicate operations directly on the data stored in the file. This reduces the amount of data that needs to be read from storage during query execution.\n",
    "\n",
    "Lightweight Indexing: ORC files include lightweight indexes that help with metadata operations and skip scanning of irrelevant data blocks when processing queries.\n",
    "\n",
    "Schema Evolution: ORC supports schema evolution, allowing you to add, remove, or modify columns in your datasets while maintaining compatibility with existing data.\n",
    "\n",
    "Performance: ORC is designed for high performance and is optimized for both read and write operations. It is particularly well-suited for complex queries on large datasets.\n",
    "\n",
    "Compatibility: ORC is commonly used in the Hadoop ecosystem, and many data processing tools and frameworks have built-in support for reading and writing ORC files.\n",
    "\n",
    "ORC files are used primarily in data warehousing, data lakes, and analytics applications, where efficiency in storage and query performance is crucial. They are often a preferred format for storing large datasets in distributed computing environments, such as Hadoop clusters, due to their performance benefits and compatibility with various data processing tools.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac34a74",
   "metadata": {},
   "source": [
    "## Create a dummy ORC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03d421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd654d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pip findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c35d2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a Spark session\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check the SparkContext\u001b[39;00m\n\u001b[0;32m     10\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "\n",
    "# Check the SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Verify that Spark is running\n",
    "print(sc.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7cd788",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize a Spark session\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDummyORC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with sample data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngineering\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarketing\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"DummyORC\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [(\"John\", 30, \"HR\"), (\"Alice\", 28, \"Engineering\"), (\"Bob\", 35, \"Marketing\")]\n",
    "columns = [\"name\", \"age\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write the DataFrame to an ORC file\n",
    "df.write.orc(\"dummy.orc\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"Dummy ORC file created: dummy.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49e439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d15d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107c293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17d662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f1341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9737d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
