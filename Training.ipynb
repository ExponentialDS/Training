{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beae9b1b-2df4-4f43-8d00-f3cda89a076c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e8405-8c62-43d3-b2f8-1ba48254a2a7",
   "metadata": {},
   "source": [
    "The most common file formats used for various types of data and purposes include:\n",
    "\n",
    "1. **Text Formats**:\n",
    "   - **Plain Text (.txt)**: Simple, human-readable text files used for various purposes.\n",
    "   - **Comma-Separated Values (.csv)**: Tabular data format with values separated by commas, widely used for data exchange.\n",
    "\n",
    "2. **Document Formats**:\n",
    "   - **Microsoft Word (.docx)**: Proprietary word processing format for Microsoft Word documents.\n",
    "   - **Portable Document Format (.pdf)**: Universal format for preserving document formatting.\n",
    "\n",
    "3. **Spreadsheet Formats**:\n",
    "   - **Microsoft Excel (.xlsx)**: Proprietary spreadsheet format for Microsoft Excel workbooks.\n",
    "   - **Comma-Separated Values (.csv)**: Also used for tabular data in spreadsheets.\n",
    "\n",
    "4. **Image Formats**:\n",
    "   - **JPEG (.jpg)**: Common compressed image format for photographs and graphics.\n",
    "   - **PNG (.png)**: Lossless image format suitable for images with transparency.\n",
    "   - **GIF (.gif)**: Format for animations and simple graphics.\n",
    "   - **BMP (.bmp)**: Bitmap format for storing graphics.\n",
    "\n",
    "5. **Audio Formats**:\n",
    "   - **MP3 (.mp3)**: Popular compressed audio format for music and audio files.\n",
    "   - **WAV (.wav)**: Uncompressed audio format with high quality.\n",
    "   - **FLAC (.flac)**: Lossless audio format for high-quality audio.\n",
    "\n",
    "6. **Video Formats**:\n",
    "   - **MP4 (.mp4)**: Common multimedia container format for video and audio.\n",
    "   - **AVI (.avi)**: Audio Video Interleave format for video files.\n",
    "   - **MKV (.mkv)**: Multimedia container format known for supporting multiple audio and subtitle tracks.\n",
    "\n",
    "7. **Archive Formats**:\n",
    "   - **ZIP (.zip)**: Archive format for compressing and packaging files.\n",
    "   - **RAR (.rar)**: Archive format with advanced compression and splitting capabilities.\n",
    "   - **TAR (.tar)**: Archive format often used on Unix-like systems.\n",
    "\n",
    "8. **Database Formats**:\n",
    "   - **SQLite (.sqlite, .db)**: Self-contained, serverless database format.\n",
    "   - **MySQL (.sql)**: SQL script files for MySQL databases.\n",
    "   - **JSON (.json)**: Semi-structured data format often used for configuration and NoSQL databases.\n",
    "\n",
    "9. **Web Formats**:\n",
    "   - **HTML (.html)**: Hypertext Markup Language for web pages.\n",
    "   - **XML (.xml)**: Extensible Markup Language for structured data.\n",
    "   - **JSON (.json)**: JavaScript Object Notation for data interchange.\n",
    "\n",
    "10. **Data Exchange Formats**:\n",
    "    - **JSON (.json)**: Common for data interchange between applications and web services.\n",
    "    - **XML (.xml)**: Used for structured data interchange, including web services.\n",
    "\n",
    "These file formats cover a wide range of data types and are commonly used in various industries and applications. The choice of format often depends on the specific use case and requirements, such as data storage, data exchange, multimedia content, or document processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af301-0caf-4425-8b71-f46b050657db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing different types of files in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0585ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a194e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'example.feather',\n",
       " 'my_dataset.h5',\n",
       " 'sample_dataset.html',\n",
       " 'sample_dataset.json',\n",
       " 'Training.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dc4c7",
   "metadata": {},
   "source": [
    "A delimiter is one or more characters that separate text strings. Common delimiters are commas (,), semicolon (;), quotes ( \", ' ), braces ( {}), pipes (|), or slashes ( / \\ ). When a program stores sequential or tabular data, it delimits each item of data with a predefined character. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdce08f",
   "metadata": {},
   "source": [
    "### 1.1 Going under the Hood of pandas read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692041ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdelimiter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mheader\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"int | Sequence[int] | None | Literal['infer']\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Sequence[Hashable] | None | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mindex_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'IndexLabel | Literal[False] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DtypeArg | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CSVEngine | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtrue_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfalse_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskipinitialspace\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskipfooter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnrows\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkeep_default_na\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mna_filter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskip_blank_lines\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mparse_dates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | Sequence[Hashable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkeep_date_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdate_parser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdayfirst\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcache_dates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CompressionOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mthousands\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoding_errors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdialect\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | csv.Dialect | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mon_bad_lines\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdelim_whitespace\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmemory_map\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfloat_precision\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Literal['high', 'legacy'] | None\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'StorageOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype_backend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DtypeBackend | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame | TextFileReader'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Read a comma-separated values (csv) file into DataFrame.\n",
       "\n",
       "Also supports optionally iterating or breaking of the file\n",
       "into chunks.\n",
       "\n",
       "Additional help can be found in the online docs for\n",
       "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "filepath_or_buffer : str, path object or file-like object\n",
       "    Any valid string path is acceptable. The string could be a URL. Valid\n",
       "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
       "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
       "\n",
       "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
       "\n",
       "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
       "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
       "sep : str, default ','\n",
       "    Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
       "    the separator, but the Python parsing engine can, meaning the latter will\n",
       "    be used and automatically detect the separator by Python's builtin sniffer\n",
       "    tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
       "    different from ``'\\s+'`` will be interpreted as regular expressions and\n",
       "    will also force the use of the Python parsing engine. Note that regex\n",
       "    delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
       "delimiter : str, default ``None``\n",
       "    Alias for sep.\n",
       "header : int, list of int, None, default 'infer'\n",
       "    Row number(s) to use as the column names, and the start of the\n",
       "    data.  Default behavior is to infer the column names: if no names\n",
       "    are passed the behavior is identical to ``header=0`` and column\n",
       "    names are inferred from the first line of the file, if column\n",
       "    names are passed explicitly then the behavior is identical to\n",
       "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
       "    replace existing names. The header can be a list of integers that\n",
       "    specify row locations for a multi-index on the columns\n",
       "    e.g. [0,1,3]. Intervening rows that are not specified will be\n",
       "    skipped (e.g. 2 in this example is skipped). Note that this\n",
       "    parameter ignores commented lines and empty lines if\n",
       "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
       "    data rather than the first line of the file.\n",
       "names : array-like, optional\n",
       "    List of column names to use. If the file contains a header row,\n",
       "    then you should explicitly pass ``header=0`` to override the column names.\n",
       "    Duplicates in this list are not allowed.\n",
       "index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
       "  Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
       "  string name or column index. If a sequence of int / str is given, a\n",
       "  MultiIndex is used.\n",
       "\n",
       "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
       "  column as the index, e.g. when you have a malformed file with delimiters at\n",
       "  the end of each line.\n",
       "usecols : list-like or callable, optional\n",
       "    Return a subset of the columns. If list-like, all elements must either\n",
       "    be positional (i.e. integer indices into the document columns) or strings\n",
       "    that correspond to column names provided either by the user in `names` or\n",
       "    inferred from the document header row(s). If ``names`` are given, the document\n",
       "    header row(s) are not taken into account. For example, a valid list-like\n",
       "    `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
       "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
       "    To instantiate a DataFrame from ``data`` with element order preserved use\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
       "    in ``['foo', 'bar']`` order or\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
       "    for ``['bar', 'foo']`` order.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the column\n",
       "    names, returning names where the callable function evaluates to True. An\n",
       "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
       "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
       "    parsing time and lower memory usage.\n",
       "dtype : Type name or dict of column -> type, optional\n",
       "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
       "    'c': 'Int64'}\n",
       "    Use `str` or `object` together with suitable `na_values` settings\n",
       "    to preserve and not interpret dtype.\n",
       "    If converters are specified, they will be applied INSTEAD\n",
       "    of dtype conversion.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "\n",
       "        Support for defaultdict was added. Specify a defaultdict as input where\n",
       "        the default determines the dtype of the columns which are not explicitly\n",
       "        listed.\n",
       "engine : {'c', 'python', 'pyarrow'}, optional\n",
       "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
       "    is currently more feature-complete. Multithreading is currently only supported by\n",
       "    the pyarrow engine.\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
       "        are unsupported, or may not work correctly, with this engine.\n",
       "converters : dict, optional\n",
       "    Dict of functions for converting values in certain columns. Keys can either\n",
       "    be integers or column labels.\n",
       "true_values : list, optional\n",
       "    Values to consider as True in addition to case-insensitive variants of \"True\".\n",
       "false_values : list, optional\n",
       "    Values to consider as False in addition to case-insensitive variants of \"False\".\n",
       "skipinitialspace : bool, default False\n",
       "    Skip spaces after delimiter.\n",
       "skiprows : list-like, int or callable, optional\n",
       "    Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
       "    at the start of the file.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the row\n",
       "    indices, returning True if the row should be skipped and False otherwise.\n",
       "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
       "skipfooter : int, default 0\n",
       "    Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
       "nrows : int, optional\n",
       "    Number of rows of file to read. Useful for reading pieces of large files.\n",
       "na_values : scalar, str, list-like, or dict, optional\n",
       "    Additional strings to recognize as NA/NaN. If dict passed, specific\n",
       "    per-column NA values.  By default the following values are interpreted as\n",
       "    NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
       "    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n",
       "    'n/a', 'nan', 'null'.\n",
       "keep_default_na : bool, default True\n",
       "    Whether or not to include the default NaN values when parsing the data.\n",
       "    Depending on whether `na_values` is passed in, the behavior is as follows:\n",
       "\n",
       "    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
       "      is appended to the default NaN values used for parsing.\n",
       "    * If `keep_default_na` is True, and `na_values` are not specified, only\n",
       "      the default NaN values are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are specified, only\n",
       "      the NaN values specified `na_values` are used for parsing.\n",
       "    * If `keep_default_na` is False, and `na_values` are not specified, no\n",
       "      strings will be parsed as NaN.\n",
       "\n",
       "    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
       "    `na_values` parameters will be ignored.\n",
       "na_filter : bool, default True\n",
       "    Detect missing value markers (empty strings and the value of na_values). In\n",
       "    data without any NAs, passing na_filter=False can improve the performance\n",
       "    of reading a large file.\n",
       "verbose : bool, default False\n",
       "    Indicate number of NA values placed in non-numeric columns.\n",
       "skip_blank_lines : bool, default True\n",
       "    If True, skip over blank lines rather than interpreting as NaN values.\n",
       "parse_dates : bool or list of int or names or list of lists or dict, default False\n",
       "    The behavior is as follows:\n",
       "\n",
       "    * boolean. If True -> try parsing the index.\n",
       "    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
       "      each as a separate date column.\n",
       "    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
       "      a single date column.\n",
       "    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
       "      result 'foo'\n",
       "\n",
       "    If a column or index cannot be represented as an array of datetimes,\n",
       "    say because of an unparsable value or a mixture of timezones, the column\n",
       "    or index will be returned unaltered as an object data type. For\n",
       "    non-standard datetime parsing, use ``pd.to_datetime`` after\n",
       "    ``pd.read_csv``.\n",
       "\n",
       "    Note: A fast-path exists for iso8601-formatted dates.\n",
       "infer_datetime_format : bool, default False\n",
       "    If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
       "    format of the datetime strings in the columns, and if it can be inferred,\n",
       "    switch to a faster method of parsing them. In some cases this can increase\n",
       "    the parsing speed by 5-10x.\n",
       "\n",
       "    .. deprecated:: 2.0.0\n",
       "        A strict version of this argument is now the default, passing it has no effect.\n",
       "\n",
       "keep_date_col : bool, default False\n",
       "    If True and `parse_dates` specifies combining multiple columns then\n",
       "    keep the original columns.\n",
       "date_parser : function, optional\n",
       "    Function to use for converting a sequence of string columns to an array of\n",
       "    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
       "    conversion. Pandas will try to call `date_parser` in three different ways,\n",
       "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
       "    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
       "    string values from the columns defined by `parse_dates` into a single array\n",
       "    and pass that; and 3) call `date_parser` once for each row using one or\n",
       "    more strings (corresponding to the columns defined by `parse_dates`) as\n",
       "    arguments.\n",
       "\n",
       "    .. deprecated:: 2.0.0\n",
       "       Use ``date_format`` instead, or read in as ``object`` and then apply\n",
       "       :func:`to_datetime` as-needed.\n",
       "date_format : str or dict of column -> format, default ``None``\n",
       "   If used in conjunction with ``parse_dates``, will parse dates according to this\n",
       "   format. For anything more complex,\n",
       "   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n",
       "\n",
       "   .. versionadded:: 2.0.0\n",
       "dayfirst : bool, default False\n",
       "    DD/MM format dates, international and European format.\n",
       "cache_dates : bool, default True\n",
       "    If True, use a cache of unique, converted dates to apply the datetime\n",
       "    conversion. May produce significant speed-up when parsing duplicate\n",
       "    date strings, especially ones with timezone offsets.\n",
       "\n",
       "iterator : bool, default False\n",
       "    Return TextFileReader object for iteration or getting chunks with\n",
       "    ``get_chunk()``.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       ``TextFileReader`` is a context manager.\n",
       "chunksize : int, optional\n",
       "    Return TextFileReader object for iteration.\n",
       "    See the `IO Tools docs\n",
       "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
       "    for more information on ``iterator`` and ``chunksize``.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       ``TextFileReader`` is a context manager.\n",
       "compression : str or dict, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
       "    path-like, then detect compression from the following extensions: '.gz',\n",
       "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
       "    (otherwise no compression).\n",
       "    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
       "    Set to ``None`` for no decompression.\n",
       "    Can also be a dict with key ``'method'`` set\n",
       "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'tar'``} and other\n",
       "    key-value pairs are forwarded to\n",
       "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
       "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor`` or\n",
       "    ``tarfile.TarFile``, respectively.\n",
       "    As an example, the following could be passed for Zstandard decompression using a\n",
       "    custom compression dictionary:\n",
       "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "        Added support for `.tar` files.\n",
       "\n",
       "    .. versionchanged:: 1.4.0 Zstandard support.\n",
       "\n",
       "thousands : str, optional\n",
       "    Thousands separator.\n",
       "decimal : str, default '.'\n",
       "    Character to recognize as decimal point (e.g. use ',' for European data).\n",
       "lineterminator : str (length 1), optional\n",
       "    Character to break file into lines. Only valid with C parser.\n",
       "quotechar : str (length 1), optional\n",
       "    The character used to denote the start and end of a quoted item. Quoted\n",
       "    items can include the delimiter and it will be ignored.\n",
       "quoting : int or csv.QUOTE_* instance, default 0\n",
       "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
       "    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
       "doublequote : bool, default ``True``\n",
       "   When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
       "   whether or not to interpret two consecutive quotechar elements INSIDE a\n",
       "   field as a single ``quotechar`` element.\n",
       "escapechar : str (length 1), optional\n",
       "    One-character string used to escape other characters.\n",
       "comment : str, optional\n",
       "    Indicates remainder of line should not be parsed. If found at the beginning\n",
       "    of a line, the line will be ignored altogether. This parameter must be a\n",
       "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
       "    fully commented lines are ignored by the parameter `header` but not by\n",
       "    `skiprows`. For example, if ``comment='#'``, parsing\n",
       "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
       "    treated as the header.\n",
       "encoding : str, optional, default \"utf-8\"\n",
       "    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
       "    standard encodings\n",
       "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "       When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
       "       ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
       "       This behavior was previously only the case for ``engine=\"python\"``.\n",
       "\n",
       "    .. versionchanged:: 1.3.0\n",
       "\n",
       "       ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
       "       influence on how encoding errors are handled.\n",
       "\n",
       "encoding_errors : str, optional, default \"strict\"\n",
       "    How encoding errors are treated. `List of possible values\n",
       "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "dialect : str or csv.Dialect, optional\n",
       "    If provided, this parameter will override values (default or not) for the\n",
       "    following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
       "    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
       "    override values, a ParserWarning will be issued. See csv.Dialect\n",
       "    documentation for more details.\n",
       "on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
       "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
       "    Allowed values are :\n",
       "\n",
       "        - 'error', raise an Exception when a bad line is encountered.\n",
       "        - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
       "        - 'skip', skip bad lines without raising or warning when they are encountered.\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        - callable, function with signature\n",
       "          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
       "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
       "          If the function returns ``None``, the bad line will be ignored.\n",
       "          If the function returns a new list of strings with more elements than\n",
       "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
       "          Only supported when ``engine=\"python\"``\n",
       "\n",
       "delim_whitespace : bool, default False\n",
       "    Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
       "    used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
       "    is set to True, nothing should be passed in for the ``delimiter``\n",
       "    parameter.\n",
       "low_memory : bool, default True\n",
       "    Internally process the file in chunks, resulting in lower memory use\n",
       "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
       "    types either set False, or specify the type with the `dtype` parameter.\n",
       "    Note that the entire file is read into a single DataFrame regardless,\n",
       "    use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
       "    (Only valid with C parser).\n",
       "memory_map : bool, default False\n",
       "    If a filepath is provided for `filepath_or_buffer`, map the file object\n",
       "    directly onto memory and access the data directly from there. Using this\n",
       "    option can improve performance because there is no longer any I/O overhead.\n",
       "float_precision : str, optional\n",
       "    Specifies which converter the C engine should use for floating-point\n",
       "    values. The options are ``None`` or 'high' for the ordinary converter,\n",
       "    'legacy' for the original lower precision pandas converter, and\n",
       "    'round_trip' for the round-trip converter.\n",
       "\n",
       "    .. versionchanged:: 1.2\n",
       "\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "    .. versionadded:: 1.2\n",
       "\n",
       "dtype_backend : {\"numpy_nullable\", \"pyarrow\"}, defaults to NumPy backed DataFrames\n",
       "    Which dtype_backend to use, e.g. whether a DataFrame should have NumPy\n",
       "    arrays, nullable dtypes are used for all dtypes that have a nullable\n",
       "    implementation when \"numpy_nullable\" is set, pyarrow is used for all\n",
       "    dtypes if \"pyarrow\" is set.\n",
       "\n",
       "    The dtype_backends are still experimential.\n",
       "\n",
       "    .. versionadded:: 2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or TextFileReader\n",
       "    A comma-separated values (csv) file is returned as two-dimensional\n",
       "    data structure with labeled axes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
       "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
       "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\crist\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411d5dde",
   "metadata": {},
   "source": [
    "### 1.1.2 Types of files one can import from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c879b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read_clipboard',\n",
       " 'read_csv',\n",
       " 'read_excel',\n",
       " 'read_feather',\n",
       " 'read_fwf',\n",
       " 'read_gbq',\n",
       " 'read_hdf',\n",
       " 'read_html',\n",
       " 'read_json',\n",
       " 'read_orc',\n",
       " 'read_parquet',\n",
       " 'read_pickle',\n",
       " 'read_sas',\n",
       " 'read_spss',\n",
       " 'read_sql',\n",
       " 'read_sql_query',\n",
       " 'read_sql_table',\n",
       " 'read_stata',\n",
       " 'read_table',\n",
       " 'read_xml']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile(r'read')\n",
    "list(filter(regex.match, dir(pd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3310d20",
   "metadata": {},
   "source": [
    "The clipboard is a temporary storage area in your computerâ€™s memory that stores the information you copy or cut. The information can be text, images, or other types of data. You can then paste the contents of the clipboard into another location, such as a document or an email "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e20a0fc",
   "metadata": {},
   "source": [
    "## CSV vs. Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a7a91e",
   "metadata": {},
   "source": [
    "CSV stands for Comma-Separated Values, while Excel is a spreadsheet application that saves files into its own format 123. CSV files are used for storing data in tabular format and are just plain text files with values separated by commas. They can be opened with text editors (such as Notepad) and are faster to process and open. However, they cannot store other information like formatting, links, charts, pictures, etc13 On the other hand, Excel files are binary files with multiple worksheets that can store formatting and perform operations on data. They can contain symbols, links, charts, pictures, etc., and are harder to read with larger sets of data13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9686ae80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Position</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>John Smith</td>\n",
       "      <td>HR</td>\n",
       "      <td>Manager</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Analyst</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Bob Johnson</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Alice Brown</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Coordinator</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Charlie Wilson</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee ID            Name   Department              Position  Salary\n",
       "0          101      John Smith           HR               Manager   75000\n",
       "1          102        Jane Doe      Finance               Analyst   60000\n",
       "2          103     Bob Johnson  Engineering              Engineer   80000\n",
       "3          104     Alice Brown    Marketing           Coordinator   50000\n",
       "4          105  Charlie Wilson        Sales  Sales Representative   65000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Employee ID': [101, 102, 103, 104, 105],\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Alice Brown', 'Charlie Wilson'],\n",
    "    'Department': ['HR', 'Finance', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'Position': ['Manager', 'Analyst', 'Engineer', 'Coordinator', 'Sales Representative'],\n",
    "    'Salary': [75000, 60000, 80000, 50000, 65000]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "employee_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "employee_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c7f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.to_csv('Test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21520fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.to_excel('Test2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0336ae",
   "metadata": {},
   "source": [
    "## Feather files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0306f7",
   "metadata": {},
   "source": [
    "A Feather file is a binary file format for efficiently storing and sharing data between different programming languages and data analysis tools. Feather was designed to be lightweight and to optimize data transfer between languages, particularly for data analysis libraries like pandas in Python and Apache Arrow in other languages like R, Julia, and more.\n",
    "\n",
    "Feather files have a few key features:\n",
    "\n",
    "Language-Agnostic: Feather files are designed to be language-agnostic, which means you can read and write them from multiple programming languages without losing data integrity or performance.\n",
    "\n",
    "Columnar Storage: Feather stores data in a columnar format, which is often more efficient for data analysis and allows for faster read and write operations, especially when working with large datasets.\n",
    "\n",
    "Metadata: Feather files include metadata that helps describe the data, such as data types and column names, making it self-descriptive.\n",
    "\n",
    "Efficient Serialization: Feather is optimized for fast serialization and deserialization, making it suitable for reading and writing data frames or tables quickly.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def55a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Column1 Column2\n",
      "0        1       A\n",
      "1        2       B\n",
      "2        3       C\n",
      "3        4       D\n",
      "4        5       E\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'Column1': [1, 2, 3, 4, 5], 'Column2': ['A', 'B', 'C', 'D', 'E']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the DataFrame to a Feather file\n",
    "df.to_feather('example.feather')\n",
    "\n",
    "# Read the Feather file back into a DataFrame\n",
    "df_from_feather = pd.read_feather('example.feather')\n",
    "\n",
    "# Display the DataFrame read from Feather\n",
    "print(df_from_feather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80f44c",
   "metadata": {},
   "source": [
    "## Fixed width Files fwf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44d7d29",
   "metadata": {},
   "source": [
    "The `read_fwf` function in pandas is used when you have data in a fixed-width format, and you want to read that data into a DataFrame for further analysis and manipulation. Fixed-width format data is a type of plain text data where each column has a specified width, and the data within each column is aligned to those widths.\n",
    "\n",
    "Here are some common scenarios when you might use `read_fwf`:\n",
    "\n",
    "1. **Legacy Data Formats**: Fixed-width files were common in legacy systems or data sources where data was stored and exchanged in a format where each column's position was predetermined. If you need to work with such data, you would use `read_fwf` to parse and load it into a DataFrame.\n",
    "\n",
    "2. **Government Data**: Government agencies and organizations often provide data in fixed-width format files. Examples include census data, economic indicators, or demographic data.\n",
    "\n",
    "3. **Mainframe Systems**: Data exported from mainframe systems or older databases may be in fixed-width format. Reading this data with `read_fwf` can be essential for data analysis or migration to modern systems.\n",
    "\n",
    "4. **Financial Data**: Financial data, including stock market data or financial reports, may be distributed in fixed-width format. Analysts often use `read_fwf` to load this data for analysis.\n",
    "\n",
    "5. **Custom Data Export Formats**: Sometimes, organizations or systems export data in custom fixed-width formats for specific applications. If you encounter such data, `read_fwf` can help you parse and work with it.\n",
    "\n",
    "When working with fixed-width data, it's crucial to know the exact column widths and data types in the file, as these details are necessary to correctly parse the data. You'll typically need to provide the `colspecs` parameter to specify the start and end positions of each column.\n",
    "\n",
    "Here's a general use case for `read_fwf`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Define the column widths for your fixed-width file\n",
    "colspecs = [(0, 5), (5, 10), (10, 15)]  # Adjust to match your data\n",
    "\n",
    "# Read the FWF file into a DataFrame\n",
    "df = pd.read_fwf('data.fwf', colspecs=colspecs)\n",
    "\n",
    "# Perform data analysis and manipulation using the DataFrame\n",
    "```\n",
    "\n",
    "In this example, `colspecs` specifies the start and end positions of three columns in the FWF file, and `read_fwf` reads the data accordingly. You should adjust `colspecs` to match the specific formatting of your FWF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f639bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\crist\\Downloads\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad38bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0\n",
      "0  123456789\n",
      "1  987654321\n",
      "2  456789123\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.read_fwf(filepath, colspecs='infer', header=None)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780919e1",
   "metadata": {},
   "source": [
    "## Benefits of FWF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19decddb",
   "metadata": {},
   "source": [
    "Fixed-width format (FWF) files have several benefits in data storage and processing:\n",
    "\n",
    "Predictable Structure: FWF files have a fixed structure where each field occupies a specific number of characters or positions within each record. This predictability makes it easy to parse and read the data accurately.\n",
    "\n",
    "Human-Readable: FWF files are often human-readable because of their fixed-column layout. This makes it easier for people to inspect the data without the need for specialized software.\n",
    "\n",
    "Efficiency: FWF files can be more memory-efficient and faster to read and write compared to variable-width files (e.g., CSV) because there's no need for delimiters. Processing fixed-width data can be faster, especially with large datasets.\n",
    "\n",
    "Preservation of Leading Zeros: FWF files are useful for storing data where leading zeros are significant (e.g., ZIP codes, product codes, or identification numbers) because they maintain the exact character positions.\n",
    "\n",
    "Data Integrity: FWF files are less prone to data corruption due to missing or misplaced delimiters that can occur in variable-width files.\n",
    "\n",
    "Compatibility: FWF files are well-suited for integration with legacy systems or other software that expects data in a fixed-width format.\n",
    "\n",
    "Data Validation: The fixed-width format makes it easier to enforce data validation rules as data must conform to the specified column widths.\n",
    "\n",
    "Alignment: When displaying FWF data in a text editor or fixed-width font, the columns align neatly, making it easier for users to visually interpret the data.\n",
    "\n",
    "However, it's essential to consider the specific use case and requirements when choosing between FWF and other data storage formats like CSV, TSV, or JSON. FWF is most beneficial when data has a consistent and predictable structure. If your data has varying column widths or complex structures, other formats may be more suitable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f60f49",
   "metadata": {},
   "source": [
    "## Google Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33e6f74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_project_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m\n\u001b[0;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mSELECT\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m  column1,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m  some_condition\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set up the BigQuery authentication (you need to authenticate to access your BigQuery data)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# You can use your Google Cloud credentials JSON file or application default credentials.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# For application default credentials, you can use:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Use the read_gbq function to execute the query and retrieve the data into a DataFrame\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_gbq(query, project_id\u001b[38;5;241m=\u001b[39myour_project_id, dialect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Now, you can work with the data in the DataFrame 'df'\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'your_project_id' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your BigQuery SQL query as a string\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  column1,\n",
    "  column2\n",
    "FROM\n",
    "  your_project_id.your_dataset.your_table\n",
    "WHERE\n",
    "  some_condition\n",
    "\"\"\"\n",
    "\n",
    "# Set up the BigQuery authentication (you need to authenticate to access your BigQuery data)\n",
    "# You can use your Google Cloud credentials JSON file or application default credentials.\n",
    "# For application default credentials, you can use:\n",
    "# pd.read_gbq(query, project_id=your_project_id, dialect='standard')\n",
    "\n",
    "# Authenticate using your Google Cloud credentials JSON file\n",
    "# pd.read_gbq(query, project_id=your_project_id, private_key='path/to/your/credentials.json', dialect='standard')\n",
    "\n",
    "# Use the read_gbq function to execute the query and retrieve the data into a DataFrame\n",
    "df = pd.read_gbq(query, project_id=your_project_id, dialect='standard')\n",
    "\n",
    "# Now, you can work with the data in the DataFrame 'df'\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c1ca2",
   "metadata": {},
   "source": [
    "## Benefits of Google Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd9c07",
   "metadata": {},
   "source": [
    "Google BigQuery is a fully-managed, serverless, and highly scalable data warehouse and analytics platform offered by Google Cloud. It provides several benefits for organizations and data professionals:\n",
    "\n",
    "Scalability: BigQuery is designed to handle massive datasets with ease. It can automatically scale to accommodate growing data volumes, ensuring that you can run complex queries on large datasets without worrying about infrastructure limitations.\n",
    "\n",
    "Serverless: You don't need to provision or manage servers when using BigQuery. It's a serverless platform, which means Google takes care of infrastructure management, including hardware and software updates.\n",
    "\n",
    "Speed: BigQuery is known for its blazing-fast query performance. It uses a distributed architecture and columnar storage to execute queries quickly, even on petabyte-scale datasets.\n",
    "\n",
    "Cost-Effective: With BigQuery's pay-as-you-go pricing model, you only pay for the data you query and store. It eliminates the need for upfront capital expenditures and allows you to control costs effectively.\n",
    "\n",
    "SQL Support: BigQuery supports standard SQL, making it easy for data analysts and SQL developers to write and run queries without the need to learn a new query language.\n",
    "\n",
    "Integration: It integrates seamlessly with other Google Cloud services, such as Google Cloud Storage, Google Data Studio, and Google Sheets, allowing you to build end-to-end data pipelines and analytics solutions.\n",
    "\n",
    "Data Warehousing and Data Lake Capabilities: BigQuery can function as both a data warehouse and a data lake. You can store structured and semi-structured data in BigQuery tables or query data directly from external storage like Google Cloud Storage.\n",
    "\n",
    "Security: Google Cloud provides robust security features, including encryption at rest and in transit, identity and access management (IAM), and audit logs. It complies with industry standards and certifications.\n",
    "\n",
    "Real-Time Data Analysis: BigQuery supports real-time data streaming, enabling you to analyze data as it arrives, making it suitable for real-time analytics use cases.\n",
    "\n",
    "Machine Learning Integration: You can leverage Google's machine learning capabilities and services like BigQuery ML to build and deploy machine learning models directly within BigQuery.\n",
    "\n",
    "Geo-spatial and Advanced Analytics: BigQuery offers support for geospatial data and a wide range of advanced analytics functions, including window functions, machine learning, and statistical analysis.\n",
    "\n",
    "Data Sharing and Collaboration: You can easily share datasets and queries with others in your organization or externally, facilitating collaboration and data sharing.\n",
    "\n",
    "Automatic Backups and High Availability: BigQuery automatically takes care of data backups and provides high availability, ensuring that your data is safe and accessible.\n",
    "\n",
    "Cost Optimization Tools: Google Cloud provides cost optimization tools and features to help you analyze and control your BigQuery costs, making it easier to manage your budget.\n",
    "\n",
    "Community and Support: BigQuery has a large and active user community, and Google Cloud offers various levels of support, including documentation, forums, and premium support options.\n",
    "\n",
    "Overall, Google BigQuery is a powerful and versatile platform that can help organizations make data-driven decisions, gain insights from their data, and leverage the benefits of cloud computing without the complexity of managing infrastructure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec54379",
   "metadata": {},
   "source": [
    "## HDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d62ec",
   "metadata": {},
   "source": [
    "\n",
    "An HDF file, which stands for \"Hierarchical Data Format\" file, is a file format designed for storing and organizing large amounts of data. HDF files are particularly popular in the scientific and engineering communities for applications that involve complex and multidimensional datasets. Here are some key features of HDF files:\n",
    "\n",
    "Hierarchical Structure: HDF files have a hierarchical structure, similar to a file system, where data can be organized into groups and datasets. This hierarchical organization makes it easy to store and manage structured data.\n",
    "\n",
    "Support for Various Data Types: HDF supports a wide range of data types, including numerical data (integers, floats, etc.), text, images, and more. This versatility makes it suitable for a broad spectrum of applications.\n",
    "\n",
    "Compression: HDF files can be compressed, which helps reduce file size while preserving data integrity. This is particularly useful when dealing with large datasets.\n",
    "\n",
    "Portability: HDF files are designed to be platform-independent. You can create and access HDF files on various operating systems, including Windows, macOS, and Linux.\n",
    "\n",
    "Data Chunking: HDF allows data to be divided into smaller, regularly sized chunks. This can improve data access and manipulation, especially for large datasets.\n",
    "\n",
    "Metadata: You can attach metadata to HDF datasets, providing additional information about the data's content, source, and any relevant attributes.\n",
    "\n",
    "Libraries and APIs: There are libraries and APIs available for various programming languages (e.g., HDF5 for C/C++, h5py for Python) that allow developers to work with HDF files, making it easier to create, read, write, and manipulate data stored in HDF format.\n",
    "\n",
    "HDF files are commonly used in fields such as astronomy, geoscience, climate modeling, bioinformatics, and more, where researchers need to store and analyze large and complex datasets. The two major versions of HDF are HDF4 and HDF5, with HDF5 being the more modern and widely adopted version due to its improved capabilities and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e9b96",
   "metadata": {},
   "source": [
    "## Create dummy HDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf2f9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Define your dataset (a simple 2D array in this example)\n",
    "data = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "# Create an HDF5 file in write mode\n",
    "with h5py.File('my_dataset.h5', 'w') as hdf_file:\n",
    "    # Create a dataset and write data to it\n",
    "    dataset = hdf_file.create_dataset('my_data', data=data)\n",
    "\n",
    "    # Optionally, add attributes and metadata to the dataset\n",
    "    dataset.attrs['description'] = 'My sample dataset'\n",
    "    dataset.attrs['author'] = 'Your Name'\n",
    "\n",
    "# The HDF5 file is automatically closed when the 'with' block exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66376c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Closed HDF5 dataset>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bef91e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h5py._hl.dataset.Dataset"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ccbc2",
   "metadata": {},
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1307d544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('my_dataset.h5', 'r') as hdf_file:\n",
    "    # Access the dataset\n",
    "    dataset = hdf_file['my_data']\n",
    "    \n",
    "    # Convert the dataset to a NumPy array\n",
    "    data = dataset[()]\n",
    "    \n",
    "    # Print the contents of the array\n",
    "    print(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381c64e",
   "metadata": {},
   "source": [
    "## Convert HDF File to Dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832e3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "2  7  8  9\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Open the HDF5 file in read mode\n",
    "with h5py.File('my_dataset.h5', 'r') as hdf_file:\n",
    "    # Access the dataset\n",
    "    dataset = hdf_file['my_data']\n",
    "    \n",
    "    # Convert the dataset to a pandas DataFrame\n",
    "    df = pd.DataFrame(dataset[()])\n",
    "\n",
    "# Now you have the data in a pandas DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2fc33",
   "metadata": {},
   "source": [
    "## HTML Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dea9dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset HTML file created: sample_dataset.html\n"
     ]
    }
   ],
   "source": [
    "# Define some example data\n",
    "data = [\n",
    "    {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"Los Angeles\"},\n",
    "    {\"name\": \"Bob\", \"age\": 35, \"city\": \"Chicago\"},\n",
    "]\n",
    "\n",
    "# Create an HTML string\n",
    "html_string = \"<html>\\n\"\n",
    "html_string += \"<head><title>Sample Dataset</title></head>\\n\"\n",
    "html_string += \"<body>\\n\"\n",
    "html_string += \"<h1>Sample Dataset</h1>\\n\"\n",
    "html_string += \"<table border='1'>\\n\"\n",
    "html_string += \"<tr><th>Name</th><th>Age</th><th>City</th></tr>\\n\"\n",
    "\n",
    "for record in data:\n",
    "    html_string += \"<tr>\"\n",
    "    html_string += f\"<td>{record['name']}</td>\"\n",
    "    html_string += f\"<td>{record['age']}</td>\"\n",
    "    html_string += f\"<td>{record['city']}</td>\"\n",
    "    html_string += \"</tr>\\n\"\n",
    "\n",
    "html_string += \"</table>\\n\"\n",
    "html_string += \"</body>\\n\"\n",
    "html_string += \"</html>\"\n",
    "\n",
    "# Save the HTML string to a file\n",
    "with open(\"sample_dataset.html\", \"w\") as html_file:\n",
    "    html_file.write(html_string)\n",
    "\n",
    "print(\"Sample dataset HTML file created: sample_dataset.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45e52a2",
   "metadata": {},
   "source": [
    "## When to parse HTML files in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ac2e8",
   "metadata": {},
   "source": [
    "You would parse HTML files in Python when you need to extract and manipulate data or information from HTML documents. Parsing HTML is common in various scenarios, including:\n",
    "\n",
    "Web Scraping: When you want to extract data from websites for purposes such as data analysis, research, or building web applications. Python libraries like Beautiful Soup and Scrapy are often used for web scraping tasks.\n",
    "\n",
    "Data Extraction: When you need to extract structured data from HTML documents, such as tables, lists, or specific elements, for further processing or analysis.\n",
    "\n",
    "Web Testing and Automation: When you automate web interactions or perform web testing, you may need to parse HTML to locate and interact with specific elements on a web page.\n",
    "\n",
    "Data Cleaning: When you have HTML-encoded content in your dataset and you want to convert it to plain text or extract specific information.\n",
    "\n",
    "Generating Dynamic Content: When you dynamically generate HTML documents based on data from databases or other sources.\n",
    "\n",
    "Web Development: When building web applications using frameworks like Flask or Django, you often work with HTML templates to render dynamic content.\n",
    "\n",
    "Python offers several libraries and tools for parsing HTML, including:\n",
    "\n",
    "Beautiful Soup: A popular library for parsing HTML and XML documents, making it easy to navigate and search the document's structure.\n",
    "\n",
    "lxml: A high-performance library for parsing XML and HTML documents. It's often used in combination with XPath for advanced data extraction.\n",
    "\n",
    "html.parser: The built-in HTML parser in Python's standard library, which can be used for basic HTML parsing tasks.\n",
    "\n",
    "Selenium: A tool often used for web testing and automation, allowing you to programmatically interact with web pages and extract data.\n",
    "\n",
    "Scrapy: A powerful web crawling and scraping framework that provides a comprehensive set of tools for extracting and processing data from websites.\n",
    "\n",
    "The specific use case for parsing HTML in Python depends on your project requirements. Whether it's extracting data from a website, cleaning and transforming HTML-encoded content, or interacting with web pages programmatically, Python offers a range of tools and libraries to help you achieve your goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f11f8",
   "metadata": {},
   "source": [
    "## JSON "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0dd25",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) files and DataFrames are both data structures used for representing and storing data, but they have different characteristics and purposes. Here are the key differences between them:\n",
    "\n",
    "Data Representation:\n",
    "\n",
    "JSON: JSON is a lightweight, text-based data interchange format. It's designed to represent structured data as a collection of key-value pairs, where keys are strings, and values can be strings, numbers, objects, arrays, booleans, or null. JSON is often used for data exchange between systems and for configuration files.\n",
    "\n",
    "DataFrame: A DataFrame is a tabular data structure commonly used in data analysis and manipulation. It's a two-dimensional, labeled data structure with rows and columns, similar to a spreadsheet or SQL table. Each column can have a different data type, making it suitable for heterogeneous data.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "JSON: JSON is typically used for data exchange between applications, configuration files, or representing data in a semi-structured format. It's commonly used in web APIs, configuration files, and data serialization.\n",
    "\n",
    "DataFrame: DataFrames are used for data analysis, data manipulation, and exploration tasks. They are a fundamental data structure in data science libraries like pandas in Python and are used for tasks such as filtering, grouping, aggregation, and visualization.\n",
    "\n",
    "Storage Format:\n",
    "\n",
    "JSON: JSON is stored as plain text and is human-readable. It's a flexible format for representing structured data, but it may not be as space-efficient as binary formats for large datasets.\n",
    "\n",
    "DataFrame: DataFrames are typically stored in memory, and they can be serialized to various formats like CSV, Excel, HDF5, or Parquet for storage. These formats may offer compression and better storage efficiency compared to JSON.\n",
    "\n",
    "Schema and Type Information:\n",
    "\n",
    "JSON: JSON does not have a predefined schema or type information. It's up to the application to interpret and validate the data.\n",
    "\n",
    "DataFrame: DataFrames have a schema that defines the data types and column names. This schema enforces type consistency within columns, making it easier to work with structured data.\n",
    "\n",
    "Access and Manipulation:\n",
    "\n",
    "JSON: Accessing and manipulating data in JSON typically involves parsing the text and working with the resulting data structure in your programming language. Libraries like json in Python are commonly used for this purpose.\n",
    "\n",
    "DataFrame: DataFrames provide a high-level API for data manipulation and analysis. Libraries like pandas offer powerful functions for filtering, transforming, aggregating, and visualizing data in a tabular format.\n",
    "\n",
    "In summary, JSON files are more suited for data interchange and configuration, while DataFrames are designed for data analysis and manipulation. The choice between the two depends on your specific use case and the type of data you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a4a6a",
   "metadata": {},
   "source": [
    "## Create a dummy JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d3d51d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"employees\": [\n",
      "        {\n",
      "            \"name\": \"John\",\n",
      "            \"age\": 30,\n",
      "            \"department\": \"HR\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Alice\",\n",
      "            \"age\": 28,\n",
      "            \"department\": \"Engineering\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Bob\",\n",
      "            \"age\": 35,\n",
      "            \"department\": \"Marketing\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Sample JSON dataset created: sample_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define a dictionary with sample data\n",
    "data = {\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"name\": \"John\",\n",
    "            \"age\": 30,\n",
    "            \"department\": \"HR\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Alice\",\n",
    "            \"age\": 28,\n",
    "            \"department\": \"Engineering\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Bob\",\n",
    "            \"age\": 35,\n",
    "            \"department\": \"Marketing\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Serialize the data to a JSON-formatted string\n",
    "json_string = json.dumps(data, indent=4)\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_string)\n",
    "\n",
    "# Alternatively, save the data to a JSON file\n",
    "with open(\"sample_dataset.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "print(\"Sample JSON dataset created: sample_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a0fc8",
   "metadata": {},
   "source": [
    "## Convert JSON to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e001adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  age   department\n",
      "0   John   30           HR\n",
      "1  Alice   28  Engineering\n",
      "2    Bob   35    Marketing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the JSON data (you can also read it from a file)\n",
    "data = {\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"name\": \"John\",\n",
    "            \"age\": 30,\n",
    "            \"department\": \"HR\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Alice\",\n",
    "            \"age\": 28,\n",
    "            \"department\": \"Engineering\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Bob\",\n",
    "            \"age\": 35,\n",
    "            \"department\": \"Marketing\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = pd.DataFrame(data[\"employees\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5688b",
   "metadata": {},
   "source": [
    "## ORC Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0553e15",
   "metadata": {},
   "source": [
    "An ORC (Optimized Row Columnar) file is a columnar storage file format used for storing and managing large volumes of structured data efficiently. It was developed as an open-source project by the Hadoop ecosystem and is widely used in big data processing frameworks like Apache Hive, Apache Spark, and Apache Impala.\n",
    "\n",
    "Key characteristics and features of ORC files include:\n",
    "\n",
    "Columnar Storage: Unlike traditional row-based storage formats, ORC files store data column by column. This allows for better compression and encoding of data because similar data types within a column can be stored together.\n",
    "\n",
    "Compression: ORC files employ various compression techniques to reduce the storage space required. They often use lightweight compression algorithms like Zlib, Snappy, or LZO.\n",
    "\n",
    "Predicate Pushdown: ORC files support predicate pushdown, which means that query engines can apply filtering and predicate operations directly on the data stored in the file. This reduces the amount of data that needs to be read from storage during query execution.\n",
    "\n",
    "Lightweight Indexing: ORC files include lightweight indexes that help with metadata operations and skip scanning of irrelevant data blocks when processing queries.\n",
    "\n",
    "Schema Evolution: ORC supports schema evolution, allowing you to add, remove, or modify columns in your datasets while maintaining compatibility with existing data.\n",
    "\n",
    "Performance: ORC is designed for high performance and is optimized for both read and write operations. It is particularly well-suited for complex queries on large datasets.\n",
    "\n",
    "Compatibility: ORC is commonly used in the Hadoop ecosystem, and many data processing tools and frameworks have built-in support for reading and writing ORC files.\n",
    "\n",
    "ORC files are used primarily in data warehousing, data lakes, and analytics applications, where efficiency in storage and query performance is crucial. They are often a preferred format for storing large datasets in distributed computing environments, such as Hadoop clusters, due to their performance benefits and compatibility with various data processing tools.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad003668",
   "metadata": {},
   "source": [
    "## Create a dummy ORC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ecd049f-3e4f-4e19-9eed-d59ca835f9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyorc\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Define the ORC schema for your dummy data\n",
    "schema = \"struct<id:int,name:string,age:int>\"\n",
    "\n",
    "# Generate some dummy data\n",
    "def generate_dummy_data(num_records):\n",
    "    data = []\n",
    "    for _ in range(num_records):\n",
    "        data.append((random.randint(1, 100), ''.join(random.choice(string.ascii_letters) for _ in range(10)), random.randint(18, 65)))\n",
    "    return data\n",
    "\n",
    "# Specify the number of records you want\n",
    "num_records = 1000\n",
    "\n",
    "# Generate dummy data\n",
    "dummy_data = generate_dummy_data(num_records)\n",
    "\n",
    "# Open a file for writing the ORC data\n",
    "with open('dummy_data.orc', 'wb') as orc_file:\n",
    "    # Create a pyorc.Writer and write the data to the file\n",
    "    with pyorc.Writer(orc_file, schema) as writer:\n",
    "        writer.writerows(dummy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be10090d-14dd-4215-afea-a780121ed997",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68, 'uvGNBPZtIq', 46),\n",
       " (28, 'HgmBbCwqeZ', 21),\n",
       " (73, 'pulnNMsGic', 52),\n",
       " (25, 'PNyWLBnhkJ', 62),\n",
       " (37, 'jjOcGeJqQi', 35),\n",
       " (38, 'IJoklXzAuy', 65),\n",
       " (55, 'oEGMtKZYYu', 55),\n",
       " (63, 'ZKbneHlUlQ', 24),\n",
       " (93, 'FAktlamclY', 50),\n",
       " (11, 'nOPhWNUrNw', 30),\n",
       " (84, 'CbFCIKbNzA', 33),\n",
       " (33, 'sYXwMHxGaI', 33),\n",
       " (83, 'yppDcDidBO', 46),\n",
       " (91, 'yjRlQsoJDO', 20),\n",
       " (85, 'JnASZzNTwL', 29),\n",
       " (42, 'FSHhyoqmCz', 47),\n",
       " (13, 'VJYDjduoDq', 43),\n",
       " (51, 'OeEIuogFxn', 63),\n",
       " (89, 'MHUziBXXmE', 27),\n",
       " (28, 'LVHGtMsBMZ', 39),\n",
       " (34, 'xnpoGMZQjo', 18),\n",
       " (99, 'SKeHhfJOsO', 52),\n",
       " (6, 'zVqxUZClva', 53),\n",
       " (56, 'WpNUzmuZNh', 39),\n",
       " (29, 'cxFuOVlufg', 51),\n",
       " (65, 'AiFwSVxczn', 60),\n",
       " (41, 'lYlZFgBDZn', 33),\n",
       " (24, 'TXzRNSVebz', 60),\n",
       " (59, 'iHcXdeTGfv', 27),\n",
       " (64, 'UhHKtasAHQ', 45),\n",
       " (88, 'BrwvtnvPvr', 52),\n",
       " (19, 'MhAGnMDUIC', 26),\n",
       " (13, 'HVagCpYPTP', 33),\n",
       " (49, 'eXdKLYVAtH', 41),\n",
       " (100, 'KtAjwUjjOr', 38),\n",
       " (34, 'fQNhxPsnnF', 63),\n",
       " (73, 'pjfmlNHzEF', 65),\n",
       " (84, 'aeocqfPdKH', 64),\n",
       " (92, 'LatBigqGQQ', 61),\n",
       " (93, 'tuTyBxmsFj', 64),\n",
       " (99, 'JxLtJcXeEQ', 27),\n",
       " (77, 'rVEHRCtyuK', 39),\n",
       " (15, 'InxbkaduWV', 49),\n",
       " (80, 'RyVinwHuGN', 40),\n",
       " (36, 'GPIloYKZbE', 26),\n",
       " (76, 'AtPXOsHwsv', 49),\n",
       " (65, 'PiAlOMTahw', 34),\n",
       " (52, 'oNpMGDihsV', 30),\n",
       " (31, 'tcjpNXtDMD', 64),\n",
       " (25, 'pmXfrHjfLM', 35),\n",
       " (94, 'qFrIFrsSPN', 19),\n",
       " (78, 'rEyDDpNxYH', 19),\n",
       " (72, 'JbAVlrhbrc', 65),\n",
       " (83, 'BsOgMQifou', 54),\n",
       " (15, 'vLYQpwaARJ', 28),\n",
       " (41, 'dzGxvncbls', 51),\n",
       " (49, 'BkJsvtGSEy', 50),\n",
       " (57, 'HJYIIPnNay', 34),\n",
       " (99, 'ULcujpBxgC', 18),\n",
       " (71, 'FUJdfQKVsW', 61),\n",
       " (77, 'yIOBrmIaBg', 60),\n",
       " (92, 'wxqdalBrMF', 53),\n",
       " (88, 'CCLRFHIFaS', 51),\n",
       " (25, 'wFLLbchfOi', 21),\n",
       " (50, 'eHwCcuIywc', 21),\n",
       " (29, 'MIJlFKGuVK', 32),\n",
       " (70, 'pXMsJrZAdz', 43),\n",
       " (12, 'MRpfXtwvqF', 56),\n",
       " (53, 'RjuGzzPAZO', 49),\n",
       " (46, 'bBhpIJiDpI', 42),\n",
       " (67, 'AKlQsnndGb', 49),\n",
       " (1, 'GjLAZDazTf', 42),\n",
       " (81, 'qOLWsETyTI', 44),\n",
       " (26, 'KjjWVRvXIa', 31),\n",
       " (16, 'YnUXKKhVTf', 22),\n",
       " (97, 'qvcOAQulJW', 26),\n",
       " (88, 'MshkRqDsyE', 62),\n",
       " (55, 'iUJNybUSoE', 37),\n",
       " (96, 'WbPAcIHZxR', 43),\n",
       " (77, 'dplqekjcLN', 62),\n",
       " (47, 'XlLOvYsTvR', 57),\n",
       " (26, 'GNsHVWDwuE', 48),\n",
       " (38, 'LKcwnkUhlM', 52),\n",
       " (33, 'FCcpXyaeeT', 23),\n",
       " (19, 'bTlxAYRApA', 63),\n",
       " (3, 'jgtejAOfrb', 46),\n",
       " (97, 'TWiXvVPPGS', 39),\n",
       " (48, 'zEskhnhKYm', 35),\n",
       " (75, 'rFDntoAfJf', 65),\n",
       " (84, 'SdTJayBYQt', 29),\n",
       " (51, 'CNyawBuVrh', 21),\n",
       " (52, 'NECMKEHifU', 34),\n",
       " (20, 'oknlzmZTIV', 56),\n",
       " (64, 'WzawUesbYD', 48),\n",
       " (87, 'oKMEiKSCro', 42),\n",
       " (47, 'LtqFBIpjqC', 18),\n",
       " (25, 'OadmvhkKOp', 42),\n",
       " (44, 'SSBIECChRG', 33),\n",
       " (90, 'ViGEnnlDKY', 52),\n",
       " (20, 'dIysyjgotJ', 44),\n",
       " (61, 'UsEfunanab', 60),\n",
       " (52, 'PoTQTtCNOw', 33),\n",
       " (63, 'budYXCYYjh', 56),\n",
       " (21, 'JRXUWLFOwS', 39),\n",
       " (71, 'kmKzicpEPc', 50),\n",
       " (67, 'CrVjsfYXJq', 33),\n",
       " (43, 'ZFWWOkBWfg', 26),\n",
       " (26, 'mcTLgcWtsQ', 35),\n",
       " (100, 'wjCfdoMoFA', 42),\n",
       " (95, 'KdRkBTBGSV', 38),\n",
       " (7, 'xCnwsmLPJR', 24),\n",
       " (8, 'gnmvvcvtRW', 33),\n",
       " (79, 'acEFnBnOur', 56),\n",
       " (20, 'MefEbFhoVU', 32),\n",
       " (80, 'EPqMvohQJq', 65),\n",
       " (83, 'GPrDGbmDOZ', 42),\n",
       " (72, 'GrWhVFoiwe', 43),\n",
       " (74, 'cTGqzNgEcl', 26),\n",
       " (78, 'zGJGRJOhJf', 54),\n",
       " (48, 'ORYxGCeNwg', 45),\n",
       " (23, 'TblDccYBmR', 31),\n",
       " (68, 'imMZqRiaqj', 54),\n",
       " (31, 'cylqGrioeY', 63),\n",
       " (10, 'dNixEqYjEt', 29),\n",
       " (58, 'mWDzkcXmia', 20),\n",
       " (38, 'zZPfkSxhcZ', 36),\n",
       " (57, 'CDsvWPTTTc', 23),\n",
       " (6, 'xjLTicOYHB', 60),\n",
       " (77, 'ZuFJehnMZg', 23),\n",
       " (71, 'PVORkYeibq', 21),\n",
       " (49, 'VIrVkmDwzj', 41),\n",
       " (63, 'UOOIXiRBYB', 34),\n",
       " (50, 'bZFzdEAqBA', 28),\n",
       " (25, 'xTNPJKkCQT', 32),\n",
       " (89, 'AShlxqgPry', 46),\n",
       " (17, 'ktWJnUSQUm', 42),\n",
       " (17, 'emsBDzAiIh', 24),\n",
       " (12, 'HBAZohtPBY', 46),\n",
       " (88, 'cWoDiubAnR', 48),\n",
       " (88, 'jlTWKXqhUa', 56),\n",
       " (14, 'wcFdBxpfFL', 55),\n",
       " (23, 'ZcwwDlLIbO', 58),\n",
       " (88, 'KGZvbOMwLP', 21),\n",
       " (37, 'TDdYMtRPqq', 65),\n",
       " (88, 'sfMZptDemP', 24),\n",
       " (66, 'uPBBkCCMPK', 53),\n",
       " (92, 'HrfhVfwODF', 48),\n",
       " (88, 'cGBXDdshHW', 39),\n",
       " (39, 'HdArBftcVI', 32),\n",
       " (87, 'flDUePLqEt', 35),\n",
       " (56, 'nRMAWGCNvY', 35),\n",
       " (99, 'zeiufnaZDL', 44),\n",
       " (91, 'UnLuODDWqb', 45),\n",
       " (80, 'pKAaFgiIMT', 21),\n",
       " (1, 'ctoPnxJbMD', 58),\n",
       " (56, 'esTYffHwXZ', 41),\n",
       " (82, 'AOmeczcjJf', 32),\n",
       " (67, 'IayOfZuptK', 51),\n",
       " (70, 'cfnMXMgGYx', 26),\n",
       " (48, 'VSAeuUjXqn', 20),\n",
       " (55, 'MUndJWWOMd', 52),\n",
       " (17, 'ZfRVGdZFxg', 37),\n",
       " (13, 'sMeUsHNDuL', 53),\n",
       " (7, 'YfmMymJWcq', 29),\n",
       " (33, 'ZcJraFhHSk', 64),\n",
       " (36, 'xSVwrDcfrC', 61),\n",
       " (71, 'DXPtmnOFtp', 27),\n",
       " (78, 'DtzExOJehK', 49),\n",
       " (88, 'QKvudwLxuF', 52),\n",
       " (14, 'uOWEFWCYHO', 25),\n",
       " (81, 'JsFcyYIPqr', 32),\n",
       " (32, 'eXfXQpYMUQ', 49),\n",
       " (76, 'qhPvVYDzuO', 47),\n",
       " (63, 'uKBazISzxv', 26),\n",
       " (83, 'JSNbjQtYdy', 55),\n",
       " (28, 'BcsfwcOXyh', 46),\n",
       " (17, 'LoTsBlgCqZ', 47),\n",
       " (70, 'gdIVbaRiPC', 39),\n",
       " (33, 'kNycoZZwAM', 41),\n",
       " (78, 'PNlMFwHPWm', 55),\n",
       " (8, 'JBudhKmKFn', 28),\n",
       " (14, 'cuYRNyeJFu', 53),\n",
       " (13, 'VezBfndKNe', 48),\n",
       " (91, 'MCdQSFLQnQ', 27),\n",
       " (60, 'uMAuetoBpW', 30),\n",
       " (33, 'WQngSgDxBC', 30),\n",
       " (76, 'YgjxIJTVXa', 28),\n",
       " (52, 'SiZNDIUkmr', 59),\n",
       " (16, 'jZhRsXHxjA', 59),\n",
       " (2, 'GpdICVyhrA', 30),\n",
       " (90, 'PtRyNwEvgO', 63),\n",
       " (56, 'GJHEhuoCfc', 32),\n",
       " (53, 'obAyMWdAjn', 21),\n",
       " (19, 'XdotesZJht', 50),\n",
       " (93, 'MEBeCPYomG', 53),\n",
       " (1, 'uJDrTbvozU', 28),\n",
       " (7, 'dwpFPvPAvq', 55),\n",
       " (21, 'tQesanVQbd', 43),\n",
       " (63, 'FiHkNapnfM', 34),\n",
       " (7, 'XmkgKNXAdf', 48),\n",
       " (93, 'hudtadoReW', 51),\n",
       " (78, 'JfZPBCOaOP', 56),\n",
       " (8, 'kVIAPAZvrE', 18),\n",
       " (78, 'wLnUiDDhMq', 19),\n",
       " (66, 'PMBFbVVnQd', 55),\n",
       " (50, 'QhDQVjFeLn', 32),\n",
       " (98, 'dppfZJCZBq', 19),\n",
       " (65, 'KMlZpzPSHn', 49),\n",
       " (42, 'smOCypcreg', 33),\n",
       " (78, 'kpYyDXysdL', 59),\n",
       " (43, 'pmUzNhwvjS', 65),\n",
       " (96, 'nGIFEkPTkq', 48),\n",
       " (88, 'TWHVIiqBUa', 51),\n",
       " (81, 'BuvImTasCA', 56),\n",
       " (99, 'XYBvnWrcyY', 58),\n",
       " (84, 'BmWFvqynzF', 54),\n",
       " (72, 'vVwTITCLUV', 63),\n",
       " (100, 'HjzgpxDhkU', 22),\n",
       " (93, 'pSSACKdILs', 46),\n",
       " (33, 'WesaJVSlrR', 37),\n",
       " (27, 'zpwEgyaYGD', 58),\n",
       " (86, 'ThSFCFAciW', 49),\n",
       " (68, 'RgXwOEtdGp', 36),\n",
       " (50, 'hcAzTtbsjA', 25),\n",
       " (14, 'TJVdYqXBtx', 59),\n",
       " (24, 'tIgHjsINwX', 47),\n",
       " (58, 'IygsQVAghi', 38),\n",
       " (26, 'boizFVKvvB', 37),\n",
       " (64, 'DlnESLTtWP', 23),\n",
       " (28, 'RLObxTJzFe', 59),\n",
       " (86, 'iWtqBhVVyl', 63),\n",
       " (100, 'BkFGnHpwPq', 25),\n",
       " (18, 'MYiapKGGmw', 62),\n",
       " (85, 'Trksqnwrph', 27),\n",
       " (8, 'lEwWFEcyDV', 50),\n",
       " (1, 'OizTdbHGYF', 58),\n",
       " (6, 'FcJMiFVVfg', 27),\n",
       " (55, 'PqWWcVWrbp', 43),\n",
       " (32, 'yICOnsNJDQ', 40),\n",
       " (75, 'WRhyhmFIKL', 24),\n",
       " (2, 'bPZnNoovbb', 44),\n",
       " (67, 'COAZQzuYfH', 19),\n",
       " (47, 'erlwTCHXNI', 25),\n",
       " (78, 'mXvWzUOprl', 59),\n",
       " (95, 'adSRPUsVED', 37),\n",
       " (17, 'xfUoHmWNyP', 58),\n",
       " (51, 'XPSZpMGdDi', 57),\n",
       " (39, 'AwvPFJXXAr', 29),\n",
       " (46, 'Upirywezpl', 33),\n",
       " (17, 'pMSmvGkgSC', 49),\n",
       " (15, 'eCpoPekfoa', 31),\n",
       " (32, 'VQHrksMvTh', 28),\n",
       " (19, 'efBMrOkeHu', 40),\n",
       " (67, 'lecTrUpsrW', 35),\n",
       " (1, 'BNIkKlIXDb', 63),\n",
       " (69, 'zvvfDbuiFk', 18),\n",
       " (51, 'vQqCrpOBMm', 58),\n",
       " (79, 'ELYoyJmnQQ', 44),\n",
       " (26, 'nUwaVGqIyU', 24),\n",
       " (52, 'AfkSFgRNtq', 57),\n",
       " (65, 'RCoCaYCQpl', 35),\n",
       " (66, 'PpyGCndrzo', 33),\n",
       " (49, 'qXOoYpjMPU', 18),\n",
       " (84, 'sQsgAsqixt', 26),\n",
       " (51, 'AIOqtPrleZ', 41),\n",
       " (72, 'PObVvXhUjH', 49),\n",
       " (46, 'jnUYkrcYEv', 36),\n",
       " (1, 'rdCWMYcOzs', 19),\n",
       " (98, 'AwaOWbcIrE', 57),\n",
       " (13, 'qBzHDIcPMe', 24),\n",
       " (7, 'jDDsHEGhYO', 63),\n",
       " (25, 'AKuBuoUquS', 56),\n",
       " (45, 'uOFyXDKVFb', 38),\n",
       " (43, 'KaBIHbpUte', 42),\n",
       " (59, 'GGpvNEMCXk', 30),\n",
       " (49, 'rtyzUsNwhR', 44),\n",
       " (60, 'GGkRnaYlza', 64),\n",
       " (39, 'fpbdazeetY', 37),\n",
       " (17, 'jUlfPjxNrG', 23),\n",
       " (37, 'bdECYExWHG', 36),\n",
       " (85, 'BaOcAyfLhy', 30),\n",
       " (94, 'ZMpqLALwwJ', 44),\n",
       " (88, 'KRlTAnKGMh', 59),\n",
       " (87, 'qTPVmrZqWm', 42),\n",
       " (41, 'AYujLCfosW', 25),\n",
       " (90, 'DcZQvKrVhr', 45),\n",
       " (50, 'KKUHuRNGrq', 64),\n",
       " (55, 'HMDPpHkfcP', 35),\n",
       " (87, 'tzXbiisgaH', 31),\n",
       " (17, 'VYNeOkDUeM', 33),\n",
       " (84, 'wAAftiaTgG', 54),\n",
       " (18, 'aGfqkQGfbS', 61),\n",
       " (85, 'PoABkguzDD', 35),\n",
       " (19, 'cVdAgCPuTt', 47),\n",
       " (5, 'FITxQuItWH', 18),\n",
       " (21, 'ruAqCTtImb', 48),\n",
       " (11, 'bSdVSOyIDe', 50),\n",
       " (73, 'AkBHiRGUCq', 39),\n",
       " (71, 'wOqVMdmmDh', 43),\n",
       " (25, 'GQHbwMlWTa', 34),\n",
       " (96, 'IWnpNDezfI', 26),\n",
       " (49, 'rqrhDdPfNn', 49),\n",
       " (98, 'mhzEvqPneM', 45),\n",
       " (22, 'RYmajmjfpy', 51),\n",
       " (7, 'gFnTvVMzPF', 39),\n",
       " (79, 'AtdSXbrgmS', 44),\n",
       " (66, 'JwLsjrnxeB', 24),\n",
       " (33, 'zoziEeErLS', 44),\n",
       " (82, 'SGaBihoXuk', 53),\n",
       " (69, 'YttPAFcwTG', 42),\n",
       " (97, 'TWcdvPnQWZ', 25),\n",
       " (11, 'pazvOGHFab', 37),\n",
       " (69, 'OlraFkDSPk', 52),\n",
       " (96, 'KJupmnDBlJ', 33),\n",
       " (84, 'JzWOxeTEPi', 49),\n",
       " (56, 'eqaFirURFg', 54),\n",
       " (69, 'QpeaLlfGYX', 40),\n",
       " (33, 'YgmdUZkGPj', 31),\n",
       " (51, 'ZGyYZlGbcf', 32),\n",
       " (33, 'DPGCTCtEsy', 28),\n",
       " (13, 'xTxSpOuLVo', 34),\n",
       " (2, 'tEZbzTrytt', 22),\n",
       " (60, 'veTswSzMPr', 25),\n",
       " (49, 'PYXuzptrgg', 62),\n",
       " (60, 'JobYbFpgEc', 35),\n",
       " (59, 'LJtKoxGfpj', 45),\n",
       " (87, 'vgHseXHemA', 49),\n",
       " (57, 'fbvMCBUCHw', 21),\n",
       " (34, 'XSpbYxKWnP', 21),\n",
       " (82, 'OukHMSKPgv', 44),\n",
       " (31, 'xQYBVsGDJC', 51),\n",
       " (72, 'WdYunmknLZ', 25),\n",
       " (57, 'rRXUvNQdlS', 44),\n",
       " (10, 'RhotyFEQqm', 41),\n",
       " (36, 'PcxNgDuoQr', 48),\n",
       " (95, 'RcHAIuTTLm', 19),\n",
       " (77, 'OmQZhrAawC', 24),\n",
       " (86, 'IACgYqOnSL', 27),\n",
       " (83, 'NlXlGQEOAc', 53),\n",
       " (63, 'XWalIQHXfH', 42),\n",
       " (92, 'LTnPXayFZz', 19),\n",
       " (21, 'DtDgNrgDWP', 21),\n",
       " (5, 'eKpcLSCNPo', 58),\n",
       " (84, 'swEDNcObDY', 60),\n",
       " (20, 'hViYkSNYJm', 56),\n",
       " (29, 'vHIfseqjxk', 51),\n",
       " (52, 'YAObBjpkcy', 55),\n",
       " (78, 'nvZzdTtCLz', 22),\n",
       " (100, 'slqhDcJzZY', 43),\n",
       " (87, 'vHxQejkzGp', 59),\n",
       " (36, 'DOwJUbTiBr', 21),\n",
       " (74, 'RCLjwonDAp', 29),\n",
       " (64, 'tJAAFwGuQl', 28),\n",
       " (49, 'CjNJemsXtf', 37),\n",
       " (48, 'mxhyyCTVqz', 27),\n",
       " (53, 'sjlzFYKQKO', 35),\n",
       " (26, 'DiPiVyBsHL', 34),\n",
       " (12, 'vwwesCdGrS', 60),\n",
       " (94, 'EZQLDYpkPG', 54),\n",
       " (11, 'oxBJEncqEa', 48),\n",
       " (59, 'wNSwSEgOHL', 48),\n",
       " (34, 'GvJhvhpilj', 48),\n",
       " (78, 'GDQfMingFf', 52),\n",
       " (96, 'WByFNVNdaS', 35),\n",
       " (96, 'hEKuLWLHYP', 38),\n",
       " (50, 'FMSlBZbZar', 35),\n",
       " (29, 'hpbXVsGSNj', 61),\n",
       " (53, 'JXAkabFFgp', 58),\n",
       " (74, 'QoGryAGYVK', 26),\n",
       " (54, 'ThzFqeuJII', 28),\n",
       " (39, 'fEyPgKyfJe', 40),\n",
       " (87, 'aKBBkvDtHc', 22),\n",
       " (1, 'pNRrXxhoZg', 40),\n",
       " (66, 'dtqZtnTczr', 26),\n",
       " (60, 'uVVcahBqbd', 65),\n",
       " (83, 'IjUxAYfMyi', 24),\n",
       " (62, 'dRXTGhHRoI', 26),\n",
       " (14, 'BxaPkatUpz', 30),\n",
       " (9, 'eigeKghRPS', 33),\n",
       " (13, 'gmLlWHdSJM', 45),\n",
       " (29, 'MYriofsuGN', 21),\n",
       " (33, 'AeRCikZbXw', 54),\n",
       " (34, 'WxRUjSFTeX', 54),\n",
       " (89, 'EuCZedtwcT', 59),\n",
       " (8, 'KDszWIDuJn', 41),\n",
       " (10, 'NsftxUgZrI', 50),\n",
       " (6, 'gxcHMcQriG', 46),\n",
       " (31, 'UMwsKcdpWf', 62),\n",
       " (24, 'hiZANltTkD', 38),\n",
       " (87, 'oANsfLXEMn', 32),\n",
       " (37, 'FeEfVHOKRl', 62),\n",
       " (50, 'UcjpdiKtLw', 49),\n",
       " (90, 'ghmNGnkxRU', 39),\n",
       " (27, 'ClKCsZOrPm', 61),\n",
       " (32, 'lTMfeByPcI', 64),\n",
       " (3, 'eSLtWPByUp', 28),\n",
       " (3, 'QlZObhWErK', 65),\n",
       " (85, 'QhBXbECNaf', 57),\n",
       " (21, 'hkYJpibjge', 37),\n",
       " (29, 'LnrOIteaax', 19),\n",
       " (59, 'wiZQbtwQOw', 23),\n",
       " (31, 'mIXzdkBBax', 28),\n",
       " (79, 'MKNtZSQQUq', 26),\n",
       " (73, 'ixHmGUDEkA', 30),\n",
       " (66, 'YRTMpCOlSd', 63),\n",
       " (36, 'NtSbfvrQqE', 20),\n",
       " (60, 'YMHfnKKJwW', 50),\n",
       " (84, 'ILmdVQopdv', 41),\n",
       " (68, 'IYiUQCnovD', 42),\n",
       " (17, 'RsfMZIITWo', 20),\n",
       " (86, 'tHOnHjexQk', 61),\n",
       " (57, 'niypTEfXPa', 33),\n",
       " (33, 'FBEkaShgpK', 22),\n",
       " (58, 'vNGsWDlaZJ', 37),\n",
       " (59, 'CDVGQDNtVy', 52),\n",
       " (71, 'DEomyJCRsA', 48),\n",
       " (90, 'CQKHCdctOg', 65),\n",
       " (44, 'plEgMxzwcZ', 35),\n",
       " (85, 'IHSQOdrRWI', 28),\n",
       " (28, 'WPiNILsfYq', 61),\n",
       " (82, 'sopFqIUTgr', 37),\n",
       " (37, 'RNKoKjxBaL', 35),\n",
       " (29, 'HqbqvSoiFU', 38),\n",
       " (58, 'kNCnupCNwm', 60),\n",
       " (17, 'DSAnJmNMTR', 34),\n",
       " (72, 'QonWSVZGPX', 62),\n",
       " (60, 'dsgGeIsFHz', 57),\n",
       " (67, 'rcAEPEmAwT', 39),\n",
       " (64, 'NbQPMzEclH', 61),\n",
       " (45, 'OxuqRRAkkG', 18),\n",
       " (42, 'inQwnNZIft', 33),\n",
       " (98, 'wvPCansDAl', 62),\n",
       " (15, 'cwOByHsdEo', 47),\n",
       " (9, 'mAeIjkkSMW', 41),\n",
       " (21, 'WGeFlklcCF', 22),\n",
       " (44, 'SHfOICAQoP', 38),\n",
       " (61, 'SofMPXSuTa', 46),\n",
       " (30, 'KBHuOKirfF', 52),\n",
       " (5, 'msIiThcWVB', 28),\n",
       " (85, 'rSLiiLJMHX', 20),\n",
       " (98, 'jyqeoEVYVf', 42),\n",
       " (76, 'YERzAeWCTx', 64),\n",
       " (12, 'nyWlaJEiDi', 19),\n",
       " (16, 'KYfqyRWPet', 44),\n",
       " (60, 'nPBAQnyNTM', 40),\n",
       " (47, 'wunwopDycS', 23),\n",
       " (22, 'SeegiQSeKO', 18),\n",
       " (46, 'exZzpFIEPB', 53),\n",
       " (93, 'hIHRbqFqTi', 37),\n",
       " (28, 'SPfrCrqjsk', 43),\n",
       " (74, 'IqrEbvJfjr', 47),\n",
       " (57, 'FtVUClayzZ', 20),\n",
       " (76, 'cqMlMutkNx', 40),\n",
       " (91, 'TRnXAXJSbU', 28),\n",
       " (31, 'dpEHUEXvUu', 56),\n",
       " (26, 'ngjoCXSLhG', 49),\n",
       " (96, 'hRMEfiAWLW', 64),\n",
       " (47, 'xeDxSFxJES', 18),\n",
       " (85, 'MYGUOrVUIn', 47),\n",
       " (14, 'MlugYCaCkE', 35),\n",
       " (54, 'dbRMCntjMN', 51),\n",
       " (93, 'pBLVHYYpUG', 20),\n",
       " (60, 'HxUTYMErLO', 25),\n",
       " (27, 'rkDXCPVqgh', 21),\n",
       " (1, 'eGijfpuKIk', 51),\n",
       " (79, 'nleAOermjv', 45),\n",
       " (84, 'mraPliAnix', 65),\n",
       " (13, 'xLWJjAvxWw', 36),\n",
       " (22, 'JveJUKKDVa', 54),\n",
       " (54, 'kWRzyeuKvO', 27),\n",
       " (38, 'NgbwFvuwfM', 46),\n",
       " (53, 'yqfllwIxeJ', 20),\n",
       " (2, 'YaZEpwJIRh', 27),\n",
       " (45, 'xWSkekPIXb', 48),\n",
       " (67, 'EXRYroqpTU', 48),\n",
       " (33, 'cpOCMViFrm', 51),\n",
       " (97, 'kJXDLCfseu', 64),\n",
       " (10, 'JmXVWdCELU', 19),\n",
       " (97, 'hCTiYGuwvY', 25),\n",
       " (83, 'GTzPuVGYuZ', 43),\n",
       " (40, 'NXZCmDbUoy', 21),\n",
       " (39, 'fbDegeZimO', 48),\n",
       " (87, 'WarryWQRff', 56),\n",
       " (20, 'DmXDVpcgmP', 51),\n",
       " (94, 'hTbKRfRtym', 25),\n",
       " (97, 'dqxgxRjSjZ', 30),\n",
       " (90, 'cquizfCNiA', 26),\n",
       " (39, 'bVSHAVVCPt', 26),\n",
       " (98, 'RhEwaXXanV', 26),\n",
       " (91, 'KoWRTqiiQZ', 42),\n",
       " (96, 'NcnNmKTVXb', 56),\n",
       " (39, 'GpEmCyacoo', 54),\n",
       " (75, 'WUNsOQeMgG', 57),\n",
       " (41, 'hUJuQnoktt', 27),\n",
       " (86, 'lJdnrDhARM', 62),\n",
       " (65, 'RtStodEjpm', 53),\n",
       " (5, 'BWuWEgUPpW', 48),\n",
       " (85, 'ehRbvckVEp', 64),\n",
       " (51, 'KktpohMxUM', 28),\n",
       " (65, 'EDcYxiOkkY', 56),\n",
       " (8, 'tlpZPTiGcd', 59),\n",
       " (29, 'ADBVsXPaii', 52),\n",
       " (58, 'ZPipDGeXPF', 45),\n",
       " (21, 'UMCgznpSpu', 49),\n",
       " (50, 'uwfDPFddaK', 39),\n",
       " (19, 'UoIzrdaZhr', 26),\n",
       " (34, 'ZJIdwlzeTz', 47),\n",
       " (74, 'FgRaxrtHke', 23),\n",
       " (42, 'VjTqddHFtQ', 65),\n",
       " (67, 'qLyrBWuULs', 63),\n",
       " (11, 'KTYxirWckv', 58),\n",
       " (89, 'QRzGOctPCz', 22),\n",
       " (30, 'VqRZfTDAoC', 34),\n",
       " (78, 'tBIUJZHVfu', 50),\n",
       " (62, 'xPZqQslQHv', 51),\n",
       " (94, 'dTtjkhEMkh', 30),\n",
       " (15, 'MsDXgeQPAo', 54),\n",
       " (19, 'ffGttzDcQv', 47),\n",
       " (49, 'OBsopPiJcF', 49),\n",
       " (94, 'gqwajJpICn', 65),\n",
       " (29, 'YcmMofNAAr', 48),\n",
       " (51, 'uAxIPBngiG', 42),\n",
       " (49, 'FQNnpaoaVz', 60),\n",
       " (4, 'TtEoCKrkKp', 21),\n",
       " (49, 'afFCAdgbPu', 35),\n",
       " (51, 'KUhkUEqYeh', 40),\n",
       " (100, 'FYmCooSNgS', 54),\n",
       " (72, 'kwPTHxuRhS', 52),\n",
       " (66, 'MhJtEicGcm', 18),\n",
       " (77, 'HrGZZfFqDv', 27),\n",
       " (43, 'UUfDJAsIey', 38),\n",
       " (57, 'UrFoYEmVUq', 31),\n",
       " (71, 'gTpbnoYnhe', 41),\n",
       " (61, 'qgJxssEgVo', 51),\n",
       " (1, 'cQJInwivng', 56),\n",
       " (58, 'FPCjpjACPV', 58),\n",
       " (36, 'NpeBUoWtSV', 20),\n",
       " (64, 'jjLHFCFRhL', 52),\n",
       " (55, 'kGuZBcCnAN', 47),\n",
       " (77, 'HqYqsNLRkS', 42),\n",
       " (59, 'jvBdxmPtbJ', 19),\n",
       " (33, 'MsYlqeSTLE', 48),\n",
       " (42, 'awPFSVUpPn', 58),\n",
       " (43, 'PzPLCkYkeo', 43),\n",
       " (94, 'auiraMVxHQ', 38),\n",
       " (53, 'PndMALzIlZ', 38),\n",
       " (58, 'IUfQkRWrON', 45),\n",
       " (54, 'fxtzrWgGAT', 23),\n",
       " (40, 'umpGHGeAfL', 26),\n",
       " (95, 'AdYheKONgy', 58),\n",
       " (15, 'SqvbTXTCVB', 39),\n",
       " (95, 'PVUrBJjyMv', 19),\n",
       " (74, 'FFlOUyEJdo', 53),\n",
       " (52, 'oxDdlgAdmo', 57),\n",
       " (48, 'dDCIBUXArY', 24),\n",
       " (12, 'HErXKqOyfW', 65),\n",
       " (80, 'caXVeEoJwv', 65),\n",
       " (43, 'BFBoDmZByZ', 58),\n",
       " (62, 'HXPuFcdlzi', 49),\n",
       " (64, 'CwTMXuzsUJ', 61),\n",
       " (51, 'mxnLEtRgve', 46),\n",
       " (24, 'oyEhErfNoY', 58),\n",
       " (39, 'lMCPbqcfPk', 44),\n",
       " (41, 'dcBYefgcPL', 46),\n",
       " (90, 'uPpOTStYfV', 60),\n",
       " (16, 'oADCLZhPOO', 46),\n",
       " (30, 'rLwgeOscKv', 52),\n",
       " (55, 'xmrDDAJhAE', 59),\n",
       " (42, 'gixjtIrWhi', 23),\n",
       " (63, 'xiChXRlWto', 57),\n",
       " (64, 'THroShVbge', 60),\n",
       " (76, 'ujjrVBITKL', 65),\n",
       " (69, 'mEzVUVOPow', 51),\n",
       " (89, 'vsdYfENqWw', 61),\n",
       " (32, 'APDpnuNWGI', 36),\n",
       " (43, 'YqdrmlXQRN', 35),\n",
       " (86, 'ZgKRWJKcLD', 60),\n",
       " (10, 'UMuaLwmMay', 23),\n",
       " (25, 'WMexESEokX', 55),\n",
       " (2, 'BwOOXhoSIY', 42),\n",
       " (64, 'RuyMXwxfIG', 26),\n",
       " (51, 'ivQiarjujj', 42),\n",
       " (66, 'ECjEIDHcai', 36),\n",
       " (70, 'uWShxCCFnG', 49),\n",
       " (93, 'KBvKxLrBzT', 60),\n",
       " (28, 'cDhTyngqvy', 34),\n",
       " (12, 'fqThqTdyCP', 49),\n",
       " (75, 'koHyPykPfP', 53),\n",
       " (11, 'HELnYvIZwq', 57),\n",
       " (7, 'iHrLakpImU', 29),\n",
       " (40, 'zdluQncJnu', 25),\n",
       " (87, 'RcAdGZMRMy', 39),\n",
       " (43, 'cHVtvscVYJ', 48),\n",
       " (12, 'xVSKWMoeKi', 30),\n",
       " (5, 'PXnDEvnbnP', 53),\n",
       " (24, 'JZeqYFpvDg', 48),\n",
       " (33, 'SGCWjUQJZe', 56),\n",
       " (74, 'PHVDmkXgeP', 64),\n",
       " (14, 'BSabojPsww', 52),\n",
       " (28, 'HHBihAbkmP', 23),\n",
       " (3, 'XjKtmLRoqm', 26),\n",
       " (68, 'IiUxPDfkFd', 30),\n",
       " (76, 'EatppApLTG', 59),\n",
       " (59, 'OvSnAMBvfJ', 33),\n",
       " (28, 'xoKRMgZLTu', 36),\n",
       " (27, 'eCadYMuxZK', 58),\n",
       " (3, 'bxKQvjvsvV', 60),\n",
       " (8, 'zGpWOnsgnN', 34),\n",
       " (77, 'haNhZYpjMU', 57),\n",
       " (85, 'XYumIeiwrm', 50),\n",
       " (11, 'HUqBQjMiFS', 25),\n",
       " (33, 'czAauSRzpX', 19),\n",
       " (71, 'vMIyvAeoen', 24),\n",
       " (18, 'EZHuteyuuO', 44),\n",
       " (4, 'KOtAWsURap', 28),\n",
       " (49, 'fnqlCxignO', 30),\n",
       " (75, 'sSGACdKKgI', 44),\n",
       " (70, 'mDJMfuurVO', 49),\n",
       " (83, 'PtZrkyERqd', 46),\n",
       " (57, 'arHAtJsXdG', 27),\n",
       " (11, 'dbbfpJcUNH', 41),\n",
       " (95, 'TCPuZzmzox', 34),\n",
       " (28, 'aycbExmVRn', 49),\n",
       " (37, 'RtOTjnIpRS', 63),\n",
       " (71, 'JFPncwrmOZ', 45),\n",
       " (51, 'sluqRRzaSU', 26),\n",
       " (54, 'pOjpMaYqQh', 35),\n",
       " (54, 'Epbmebykir', 53),\n",
       " (99, 'WmRqvAKLJm', 46),\n",
       " (90, 'gCofrQwDeJ', 23),\n",
       " (96, 'cznjImQSSz', 34),\n",
       " (36, 'EyShGGuhLn', 33),\n",
       " (67, 'uerkhhsLjQ', 27),\n",
       " (45, 'HnoGcRzxAO', 45),\n",
       " (6, 'zbJsGyCRsu', 39),\n",
       " (38, 'KyZXjVbBmz', 21),\n",
       " (14, 'isiwHDjmux', 56),\n",
       " (18, 'bSkguBemzZ', 56),\n",
       " (2, 'wuoAyCFbEI', 51),\n",
       " (92, 'lMLvfOAZIJ', 36),\n",
       " (98, 'gJLZmqjTJn', 37),\n",
       " (7, 'kYkOqPFlRB', 56),\n",
       " (70, 'kKKaLTBLAV', 44),\n",
       " (23, 'rZQpuRykOV', 49),\n",
       " (18, 'ZrNxvkiyYw', 29),\n",
       " (68, 'xYNqWUrygr', 61),\n",
       " (74, 'DBPrMabrfE', 49),\n",
       " (54, 'GmLDpkpouC', 52),\n",
       " (30, 'LdCYGehmLk', 61),\n",
       " (54, 'XrBTnKZmUc', 31),\n",
       " (96, 'bbbcUpgRfg', 19),\n",
       " (18, 'uErobpEFly', 49),\n",
       " (9, 'QkEyTKtQoB', 48),\n",
       " (6, 'MuKTsPGNFC', 61),\n",
       " (25, 'WIuCkJdEzi', 23),\n",
       " (74, 'YHkGcwPVgD', 39),\n",
       " (99, 'ztSdjvamhv', 25),\n",
       " (25, 'zNCzGwHNrm', 65),\n",
       " (13, 'nDwHyRPOzb', 52),\n",
       " (68, 'cluUtcDWLD', 29),\n",
       " (67, 'RMMLpCgYKx', 58),\n",
       " (20, 'BGMHoQiAnc', 25),\n",
       " (53, 'EZZlSaYJqe', 46),\n",
       " (93, 'PZohDsACBC', 65),\n",
       " (30, 'DMfswwYCmF', 49),\n",
       " (22, 'tKjLMftElO', 48),\n",
       " (40, 'TYLXOMJjFD', 48),\n",
       " (92, 'dynGnnzWDN', 22),\n",
       " (51, 'uOfWRafeYc', 48),\n",
       " (97, 'vzpSQICDZW', 51),\n",
       " (95, 'ahPLJbAgdR', 18),\n",
       " (54, 'MmpfqoqTSq', 50),\n",
       " (16, 'WxvTxpycjy', 41),\n",
       " (75, 'xbOwwxtJtH', 19),\n",
       " (58, 'eqXpDFXmDU', 47),\n",
       " (71, 'wZiUSENmZN', 33),\n",
       " (5, 'dggIIPWpeD', 26),\n",
       " (35, 'VljUrWKRsV', 56),\n",
       " (16, 'UqwCErlTUB', 63),\n",
       " (19, 'UQojzMlaCA', 23),\n",
       " (37, 'tLCrYykdgd', 59),\n",
       " (68, 'oZUFNVogky', 57),\n",
       " (17, 'wXrfZnjxUj', 49),\n",
       " (14, 'yhqLOushEm', 21),\n",
       " (60, 'ttpdXBmFzf', 25),\n",
       " (14, 'KKZLobxCOi', 57),\n",
       " (3, 'WFNJSYPMEW', 64),\n",
       " (70, 'QUNnaMyOkY', 51),\n",
       " (1, 'VXaXLaPUkc', 62),\n",
       " (59, 'BzCkJxzadK', 20),\n",
       " (61, 'XZUuklxavE', 34),\n",
       " (39, 'OUgPcwGpKp', 50),\n",
       " (88, 'alLxaEPzgW', 52),\n",
       " (36, 'hbVcQHmDFy', 35),\n",
       " (76, 'OkbKjflBlg', 46),\n",
       " (43, 'zIoSazhofq', 19),\n",
       " (11, 'tFMnMyJnWC', 29),\n",
       " (13, 'FmoQJYVVAe', 62),\n",
       " (87, 'ecufxYcYfq', 34),\n",
       " (26, 'PGGwiwcXig', 60),\n",
       " (83, 'nVrmuRITWF', 33),\n",
       " (12, 'ALscpygGHa', 48),\n",
       " (14, 'PFMjxHPqgh', 27),\n",
       " (8, 'OBoqLcZrQq', 42),\n",
       " (15, 'ZuaFUKtRSW', 26),\n",
       " (42, 'ohxcTzMGxR', 42),\n",
       " (79, 'ExdmVTIgmr', 55),\n",
       " (47, 'kOFKmHVaZw', 22),\n",
       " (11, 'SjnlwmTLuR', 65),\n",
       " (30, 'CUVqQxIUgB', 44),\n",
       " (70, 'FRxiRmGOJl', 50),\n",
       " (35, 'fCEXJaJXtn', 62),\n",
       " (90, 'cfLLuCJXdO', 48),\n",
       " (84, 'jnwXSjnmSR', 60),\n",
       " (84, 'ghEEvNgHHi', 27),\n",
       " (96, 'XDAHQFNkxO', 32),\n",
       " (85, 'zSeAjStoqo', 51),\n",
       " (28, 'NYaGvftdXa', 50),\n",
       " (99, 'LKIpHYTHcw', 62),\n",
       " (53, 'jmKRJZeifr', 56),\n",
       " (23, 'dRDoMZkolF', 30),\n",
       " (91, 'BUxfRkPScd', 28),\n",
       " (59, 'UjbISxQYXn', 62),\n",
       " (12, 'qSgLPolBoF', 41),\n",
       " (3, 'nBVCwOTYPv', 35),\n",
       " (57, 'ICdnnAwSbn', 59),\n",
       " (67, 'naxlvezqhh', 63),\n",
       " (19, 'smdgWPUoDT', 45),\n",
       " (28, 'HsJAOGHnXW', 57),\n",
       " (73, 'AhQZDEIJbE', 57),\n",
       " (5, 'NbcrAinVEM', 42),\n",
       " (40, 'lRKAaquJXG', 18),\n",
       " (69, 'gJvUSxQhHY', 23),\n",
       " (57, 'LWpOKZggXj', 39),\n",
       " (31, 'tvMRNEGRxj', 51),\n",
       " (93, 'ROcVBSAdnJ', 59),\n",
       " (61, 'HbUlKGdPda', 18),\n",
       " (62, 'KZUOGCdBqj', 23),\n",
       " (6, 'HtiNkUFoHt', 40),\n",
       " (5, 'OqLkqNGJZM', 29),\n",
       " (66, 'WAPqyyqyzP', 27),\n",
       " (80, 'WUjQbyFCqs', 33),\n",
       " (53, 'UwNQpNWsHo', 20),\n",
       " (1, 'OFOzXIiXkU', 31),\n",
       " (10, 'hOjVJxSMSH', 21),\n",
       " (100, 'LqpvfGTcjg', 48),\n",
       " (81, 'uWzknSGUHO', 55),\n",
       " (14, 'RAQahLvaTy', 38),\n",
       " (28, 'NavbluzefH', 34),\n",
       " (3, 'jXdXPxrOCU', 35),\n",
       " (43, 'bCzUdatxFK', 45),\n",
       " (17, 'HyQVpHkJAh', 25),\n",
       " (39, 'vAcQlYURZy', 51),\n",
       " (11, 'HOLoWZVTKR', 49),\n",
       " (28, 'ikupebYxOd', 47),\n",
       " (27, 'WLBShhdDDF', 64),\n",
       " (1, 'tnFJbRbpJU', 64),\n",
       " (92, 'pWvRqVCHtM', 26),\n",
       " (57, 'xAlaZWEoMZ', 28),\n",
       " (6, 'YEXdWbeFAI', 63),\n",
       " (15, 'BIqzPEGvhG', 44),\n",
       " (78, 'GEkXRowdXr', 36),\n",
       " (55, 'gfxQlXIRyL', 34),\n",
       " (25, 'GtwERKoHwt', 29),\n",
       " (44, 'nWMsoUGbUR', 37),\n",
       " (20, 'dXKNaShHYD', 55),\n",
       " (86, 'OrCSWQaiOv', 50),\n",
       " (61, 'arZoFjhHju', 38),\n",
       " (87, 'TGqPpbqACg', 50),\n",
       " (49, 'cmIKkxkLTo', 55),\n",
       " (20, 'ANFSaEIirM', 40),\n",
       " (37, 'GXaerUIpSc', 54),\n",
       " (69, 'DFelZQRfWa', 64),\n",
       " (62, 'PhbGKQHxvG', 18),\n",
       " (66, 'ufUfTkEAUg', 53),\n",
       " (4, 'hiUqekQtdG', 48),\n",
       " (77, 'mrRcXOgnmd', 32),\n",
       " (48, 'AnDEPYiXDo', 23),\n",
       " (45, 'UdtVMlQafv', 54),\n",
       " (2, 'WIEozKCegu', 52),\n",
       " (3, 'SKAthYoecb', 47),\n",
       " (73, 'zgnGjSyPVe', 29),\n",
       " (79, 'qFlbhpffPN', 47),\n",
       " (98, 'mPtWLcZvvR', 46),\n",
       " (94, 'cILJYkLWaB', 38),\n",
       " (43, 'RqLTGyEXIy', 59),\n",
       " (1, 'srTwASIckQ', 29),\n",
       " (79, 'CShGnrpNRS', 20),\n",
       " (71, 'bvaMyhGpbl', 58),\n",
       " (59, 'UiDtrHqzQq', 56),\n",
       " (50, 'awcevuIHIH', 42),\n",
       " (13, 'HjXEbmbfCU', 39),\n",
       " (34, 'dPYDNHJdWx', 39),\n",
       " (99, 'ZKgXJdWEyn', 30),\n",
       " (14, 'ytSkNmCMcl', 60),\n",
       " (41, 'jXvjuvjMNE', 57),\n",
       " (29, 'FuYHGVwoXF', 55),\n",
       " (31, 'BrSxbaQWIA', 65),\n",
       " (62, 'DdefByPrYj', 65),\n",
       " (2, 'PUAbqyQAKJ', 39),\n",
       " (25, 'MDBFXQoDeQ', 36),\n",
       " (81, 'vYessjcOwH', 62),\n",
       " (9, 'bEGWXClwxJ', 43),\n",
       " (66, 'hQUYPPbKcN', 41),\n",
       " (82, 'XQxGLtsBlR', 62),\n",
       " (24, 'tSJOJkuPzp', 24),\n",
       " (88, 'oNyrosrbDj', 34),\n",
       " (65, 'hpNTVAdIFA', 33),\n",
       " (68, 'wgFYQCbUMh', 33),\n",
       " (22, 'oyfxHlVnHB', 54),\n",
       " (47, 'SUgnhPrkMd', 62),\n",
       " (25, 'PrIgiHYFtj', 36),\n",
       " (17, 'xfJtsmGFMR', 55),\n",
       " (67, 'CNTjRpTMIg', 40),\n",
       " (28, 'uJPNtrXRzV', 52),\n",
       " (15, 'NaUJwjQuKm', 33),\n",
       " (77, 'TGStzaicuP', 53),\n",
       " (100, 'TYhTyuTeMy', 61),\n",
       " (66, 'fHMqDXqiEs', 46),\n",
       " (31, 'FhBtXGDSbM', 53),\n",
       " (5, 'EHMlyTcyEz', 25),\n",
       " (64, 'JaEhBzgRWb', 60),\n",
       " (59, 'JZcERuebVI', 35),\n",
       " (64, 'bifCdfIFAu', 48),\n",
       " (81, 'IFGcqEuTtQ', 31),\n",
       " (51, 'sIGhCEMPyD', 53),\n",
       " (34, 'VkEObOwtiV', 25),\n",
       " (50, 'uAHukSylqh', 45),\n",
       " (47, 'qBalwQisQS', 33),\n",
       " (13, 'jQnZIfpBLq', 24),\n",
       " (2, 'IaiOLiGrYY', 53),\n",
       " (86, 'zsxvoopXIM', 51),\n",
       " (24, 'ygHlPWhNUk', 32),\n",
       " (3, 'tshbBsDJae', 55),\n",
       " (86, 'emdLozHqJU', 51),\n",
       " (37, 'mVCBbETkfz', 29),\n",
       " (39, 'LvTyAHbsOp', 56),\n",
       " (56, 'gmTAHdNGgb', 50),\n",
       " (1, 'MHlyBXGKJa', 33),\n",
       " (63, 'yARsVrAhxg', 39),\n",
       " (88, 'rkvGSjCyOy', 59),\n",
       " (43, 'GZHPmYECfz', 54),\n",
       " (29, 'pdtVyhvMwI', 47),\n",
       " (51, 'zqgotgKxRr', 43),\n",
       " (83, 'xCBBnATIeh', 54),\n",
       " (20, 'kjQRLHplGG', 34),\n",
       " (6, 'PIWHpyQlkb', 41),\n",
       " (36, 'NZBRioQNyN', 53),\n",
       " (15, 'qWCavIMVpS', 59),\n",
       " (2, 'NPKiJIduoh', 26),\n",
       " (84, 'tcDaZFvrqq', 61),\n",
       " (6, 'JneAMERkZX', 22),\n",
       " (94, 'eirmfSDVOp', 42),\n",
       " (36, 'hTcmvmCTjF', 33),\n",
       " (45, 'yTHtVGDTYu', 31),\n",
       " (44, 'TbiiFFHAkU', 21),\n",
       " (34, 'mrrfwypooJ', 58),\n",
       " (49, 'jNCNTeNOzi', 44),\n",
       " (28, 'YAkBMQQMDq', 34),\n",
       " (19, 'NmqLNcPSii', 51),\n",
       " (5, 'bcLenojmUx', 36),\n",
       " (49, 'SGnTMaVERt', 54),\n",
       " (18, 'cnmhDLRYoo', 40),\n",
       " (72, 'lYCumtulzs', 55),\n",
       " (19, 'VXEmPYxCER', 21),\n",
       " (98, 'ZUCSVKyenT', 45),\n",
       " (52, 'XRALNShLER', 60),\n",
       " (7, 'VpzcCqhYMI', 35),\n",
       " (70, 'ZBranoJGMS', 21),\n",
       " (93, 'OiRIiYNrQj', 61),\n",
       " (64, 'ASyNqTjOaM', 23),\n",
       " (36, 'BWUfbNOcjp', 24),\n",
       " (4, 'WLltCcYDPy', 45),\n",
       " (41, 'vYngeRecsn', 57),\n",
       " (57, 'TLzIgMTDKu', 55),\n",
       " (43, 'eHyiKQwFaH', 62),\n",
       " (80, 'ZemsYLssWz', 24),\n",
       " (23, 'NFmOMsCBnG', 55),\n",
       " (61, 'BNrFKdjOrz', 61),\n",
       " (70, 'EAIOiUCVKX', 61),\n",
       " (62, 'tIEKYZwPBf', 55),\n",
       " (54, 'DaRyeWwEPT', 51),\n",
       " (1, 'USeZKcUMLB', 29),\n",
       " (42, 'irdIxJGlHn', 33),\n",
       " (83, 'yKHcsILwbH', 24),\n",
       " (77, 'aJbhWMgPgg', 52),\n",
       " (90, 'MTFsUjqGcA', 60),\n",
       " (28, 'yVBrmIHoTw', 45),\n",
       " (34, 'oHdMcXrOgc', 35),\n",
       " (3, 'ValeKJUDbz', 21),\n",
       " (2, 'pMKuAKrljt', 38),\n",
       " (28, 'FtnFMuBfzL', 63),\n",
       " (6, 'EoYsHMAMKy', 35),\n",
       " (31, 'zmsVYrfguB', 59),\n",
       " (36, 'nFqMPFcoFM', 46),\n",
       " (73, 'ehYJLVizZp', 58),\n",
       " (18, 'rxhaorNjkk', 19),\n",
       " (86, 'EwsJdKbnBl', 42),\n",
       " (77, 'CMLkNoQtUx', 27),\n",
       " (46, 'PDUNaiYlLO', 36),\n",
       " (12, 'cEDNWiOrLf', 47),\n",
       " (97, 'GlmQmsSKVh', 19),\n",
       " (47, 'uwcwLdfCKy', 37),\n",
       " (34, 'NvoBpijpSL', 34),\n",
       " (30, 'QEGYmmQGaa', 20),\n",
       " (79, 'RJgehDGtNa', 32),\n",
       " (46, 'uVmUwIUllh', 24),\n",
       " (90, 'IiJbZvZDLm', 23),\n",
       " (16, 'PhGOnZgrQm', 18),\n",
       " (68, 'PxyRXYbVlH', 33),\n",
       " (79, 'RrwAyFubiX', 65),\n",
       " (9, 'nzXhJEkYAV', 29),\n",
       " (5, 'QDyiqFVPYf', 64),\n",
       " (6, 'EqktbKIjLJ', 29),\n",
       " (54, 'gtpaYgvlyr', 39),\n",
       " (84, 'SKZTAUuJHr', 31),\n",
       " (43, 'TDLGVJpgNR', 33),\n",
       " (75, 'NpfPBQDBAm', 26),\n",
       " (89, 'BQtuJIzJDd', 40),\n",
       " (55, 'IZDTWSGhGX', 61),\n",
       " (90, 'PayhwTrsQC', 56),\n",
       " (58, 'LFVQNHxPky', 22),\n",
       " (69, 'vbUsKiKnTP', 61),\n",
       " (37, 'oNqlJFrJzq', 24),\n",
       " (54, 'wLNYYuFpFq', 19),\n",
       " (71, 'HdkKfSXUwO', 18),\n",
       " (3, 'lImPamrgeg', 52),\n",
       " (17, 'RogUEcowJC', 21),\n",
       " (92, 'VCgPlMXZuB', 25),\n",
       " (81, 'GfYXwIRFZl', 22),\n",
       " (77, 'lLLFsPGwgq', 64),\n",
       " (30, 'rNBynnQuRj', 55),\n",
       " (58, 'iUrKRlogBU', 65),\n",
       " (41, 'FcaTwUBTyx', 43),\n",
       " (21, 'qsOOWDHAwK', 41),\n",
       " (73, 'btnuxhfBLF', 26),\n",
       " (34, 'TqkCfEqhVr', 59),\n",
       " (1, 'kaNijQXnIP', 57),\n",
       " (76, 'JqCgxjeimD', 65),\n",
       " (95, 'uVDFKXBlTz', 50),\n",
       " (75, 'vtsUdnnIvE', 45),\n",
       " (4, 'BzRqLDzbxi', 24),\n",
       " (66, 'yqjEHBAtVI', 47),\n",
       " (77, 'jZeicwccLg', 21),\n",
       " (48, 'NXVGNSHUeE', 41),\n",
       " (28, 'jifTunHhZx', 43),\n",
       " (16, 'cIpyDGVhMM', 42),\n",
       " (2, 'TYdPYRFsId', 31),\n",
       " (93, 'XbWShSTOUM', 46),\n",
       " (19, 'LHDNkgFDQL', 64),\n",
       " (80, 'VCNcEYaEas', 61),\n",
       " (77, 'AIdKMDuTYZ', 60),\n",
       " (70, 'zkSZoYXwXf', 32),\n",
       " (92, 'exuqttqNJL', 51),\n",
       " (77, 'xEFInwOIyt', 18),\n",
       " (33, 'qdwZHicgwH', 48),\n",
       " (48, 'mVreovjgAL', 46),\n",
       " (17, 'bQOXRzGUYc', 41),\n",
       " (62, 'HzNKEdUxeH', 31),\n",
       " (21, 'ntJtVMhnbQ', 29),\n",
       " (79, 'slldUaFodA', 21),\n",
       " (34, 'bPnhAetbpB', 34),\n",
       " (3, 'YOANVWoDsS', 49),\n",
       " (50, 'oRooXNpIul', 49),\n",
       " (31, 'XbBhctNcFz', 47),\n",
       " (29, 'dbapuMlZuM', 18),\n",
       " (17, 'muDluJzWou', 48),\n",
       " (37, 'lcZszNEapf', 35),\n",
       " (45, 'AyNAKYPvUf', 26),\n",
       " (48, 'ktoPMdVAai', 38),\n",
       " (5, 'ObLRffKzuy', 42),\n",
       " (2, 'alttkesLDN', 65),\n",
       " (79, 'AXDZwWSKpO', 31),\n",
       " (73, 'VxIfdBhbNU', 23),\n",
       " (62, 'oKghvnXSmt', 60),\n",
       " (76, 'qTyfkuiNTD', 37),\n",
       " (19, 'iJkKdlnPBc', 22),\n",
       " (35, 'cKpshltQqo', 44),\n",
       " (30, 'BzjEaOpsYw', 63),\n",
       " (81, 'kpneOxqqah', 18),\n",
       " (13, 'FzJmJQElwL', 50),\n",
       " (25, 'uRHeAPYrXd', 62),\n",
       " (7, 'yIgmmOuDor', 55),\n",
       " (9, 'mlvNNdlWDR', 47),\n",
       " (35, 'zAnDVwzLld', 28),\n",
       " (59, 'VtzlSPcmnB', 31),\n",
       " (70, 'zMxuGGwqsd', 31),\n",
       " (34, 'JIIJHMnBiU', 42),\n",
       " (19, 'PyNVJnVZxx', 31),\n",
       " (42, 'lXCSzyYDeX', 18),\n",
       " (79, 'UCZbMEtgfA', 40),\n",
       " (67, 'btcJxceehR', 43),\n",
       " (3, 'kJulGhpgGn', 20),\n",
       " (91, 'twZRAHtMSB', 32),\n",
       " (48, 'cXdMYIwJNp', 37),\n",
       " (16, 'HVZYtUmoVq', 41),\n",
       " (68, 'mdlXFGOLgt', 59),\n",
       " (23, 'AVyPcqWjnO', 39),\n",
       " (75, 'SHrxsNxTLi', 53),\n",
       " (49, 'ztlpMUtjMi', 38)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e801aaf-8f9c-445c-a284-a22ee440712b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b3317e-694c-4a69-9758-3af5207a3064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "orc_test = pd.read_orc('dummy_data.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d3d983-cb53-4ad6-bef2-f4f7a2ad9aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ba5ceac-e3d0-440b-8710-16b43d393039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>uvGNBPZtIq</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>HgmBbCwqeZ</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>pulnNMsGic</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>PNyWLBnhkJ</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>jjOcGeJqQi</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>16</td>\n",
       "      <td>HVZYtUmoVq</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>68</td>\n",
       "      <td>mdlXFGOLgt</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>23</td>\n",
       "      <td>AVyPcqWjnO</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>75</td>\n",
       "      <td>SHrxsNxTLi</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>49</td>\n",
       "      <td>ztlpMUtjMi</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        name  age\n",
       "0    68  uvGNBPZtIq   46\n",
       "1    28  HgmBbCwqeZ   21\n",
       "2    73  pulnNMsGic   52\n",
       "3    25  PNyWLBnhkJ   62\n",
       "4    37  jjOcGeJqQi   35\n",
       "..   ..         ...  ...\n",
       "995  16  HVZYtUmoVq   41\n",
       "996  68  mdlXFGOLgt   59\n",
       "997  23  AVyPcqWjnO   39\n",
       "998  75  SHrxsNxTLi   53\n",
       "999  49  ztlpMUtjMi   38\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74776426-34d4-458b-9514-608e291d525b",
   "metadata": {},
   "source": [
    "## Spark and Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4d7d4-0d78-4966-9109-b6383ecf035b",
   "metadata": {},
   "source": [
    "Apache Spark:\n",
    "\n",
    "General-purpose Data Processing: Spark is a general-purpose, distributed data processing framework that can handle a wide range of data processing tasks, including data analysis, machine learning, and more.\n",
    "In-memory Processing: Spark performs in-memory processing, which can make it faster for iterative algorithms and interactive data analysis.\n",
    "Programming in Scala, Java, Python, or R: You can write Spark applications using various programming languages.\n",
    "If you have complex data processing and analysis needs, especially when dealing with large-scale data, Spark can be a powerful choice.\n",
    "\n",
    "Apache Hive:\n",
    "\n",
    "SQL-like Query Language: Hive provides a SQL-like query language called HiveQL, which makes it easy for analysts and SQL users to work with large datasets.\n",
    "Data Warehousing: Hive is often used for data warehousing tasks, where you need to structure and query data stored in a data lake or data warehouse.\n",
    "Integration with Hadoop Ecosystem: Hive integrates well with the Hadoop ecosystem and can work with various file formats, including ORC, Parquet, and others.\n",
    "If you primarily need to query and analyze structured data using SQL-like queries and work with data stored in ORC or other columnar formats, Hive can be a good choice.\n",
    "\n",
    "In the context of ORC files, both Spark and Hive can be used to analyze them, but the choice depends on the complexity of your analysis, your familiarity with the tools, and your overall data processing architecture. Spark might be more suitable for complex data transformations and machine learning tasks, while Hive is often used for straightforward SQL-like queries and data warehousing tasks.\n",
    "\n",
    "Ultimately, the choice between Spark and Hive (or any other tool) should align with your specific use case, data volume, and the skills of your team. You can also integrate Spark and Hive if needed, allowing you to leverage the strengths of both tools in your data analysis workflows.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a476b-55a4-46d2-a261-10cdabf03f03",
   "metadata": {},
   "source": [
    "## Other tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769141e3-d50d-41e4-adff-8222be28de6a",
   "metadata": {},
   "source": [
    "Several tools and libraries can be used to analyze ORC (Optimized Row Columnar) files, depending on your specific needs and preferences. Here are some commonly used tools and libraries for working with ORC files:\n",
    "\n",
    "Apache Hive:\n",
    "\n",
    "As mentioned earlier, Apache Hive provides a SQL-like query language (HiveQL) that allows you to query and analyze ORC files along with other data formats. It is particularly well-suited for structured data analysis.\n",
    "Apache Pig:\n",
    "\n",
    "Apache Pig is a platform for analyzing large data sets. You can use the Pig Latin scripting language to load and process ORC files.\n",
    "Apache Impala:\n",
    "\n",
    "Apache Impala is a massively parallel processing SQL query engine for data stored in Hadoop. It supports ORC files and enables interactive SQL queries.\n",
    "Apache Drill:\n",
    "\n",
    "Apache Drill is a distributed SQL query engine that supports a variety of data sources, including ORC files. It allows you to run ANSI SQL queries on ORC data without the need for ETL.\n",
    "Apache Parquet:\n",
    "\n",
    "While Parquet is a different columnar file format, it is often used alongside ORC. Many tools that support Parquet also support ORC. Parquet is popular for its performance and columnar storage characteristics.\n",
    "Python Libraries:\n",
    "\n",
    "You can use Python libraries like PyORC and Pandas with the pyarrow library to read and analyze ORC files in Python. These libraries allow you to work with ORC data in a Python environment.\n",
    "Apache Arrow:\n",
    "\n",
    "Apache Arrow is a cross-language development platform for in-memory data that includes support for reading and writing ORC files. It can be used with various programming languages.\n",
    "Big Data Platforms:\n",
    "\n",
    "If you're working within a big data platform like Databricks, Amazon EMR, Google Dataprep, or other cloud-based services, these platforms often provide tools and APIs for working with ORC files as part of their data processing capabilities.\n",
    "Custom Applications:\n",
    "\n",
    "You can also build custom applications using programming languages like Java, Scala, or C++ by leveraging ORC file reader libraries and integrating them into your data processing pipelines.\n",
    "The choice of tool or library depends on your specific use case, programming language preference, data volume, and existing data infrastructure. Many of these tools support multiple file formats, including ORC, Parquet, and more, so you can choose the one that best fits your data and analysis needs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ffb26-a6fc-4838-b2cd-2dc3c784bb08",
   "metadata": {},
   "source": [
    "## Start Pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20da1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ee7aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pip findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42068e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "\n",
    "# Check the SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Verify that Spark is running\n",
    "print(sc.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8041b-3f9d-469f-b6ca-7eea863077a1",
   "metadata": {},
   "source": [
    "## Paraquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a791a-7b9e-408e-9549-151d3100ef05",
   "metadata": {},
   "source": [
    "### Parquet:\n",
    "\n",
    "- Columnar storage format optimized for analytics.\n",
    "- Binary format with schema preservation and compression.\n",
    "- Ideal for big data and analytical processing.\n",
    "\n",
    "### CSV (Comma-Separated Values):\n",
    "\n",
    "- Text-based format with human-readability.\n",
    "- Lacks schema information and supports limited data types.\n",
    "- Common for simple data exchange between applications.\n",
    "\n",
    "### Excel:\n",
    "\n",
    "- Proprietary spreadsheet application, not a file format.\n",
    "- Supports rich formatting, but not suitable for big data.\n",
    "- Used for interactive data analysis with formulas, charts, et"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2437ae-04c8-4e56-b843-f39f8c360637",
   "metadata": {},
   "source": [
    "##  Create dummy Paraquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aad3f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame with dummy data\n",
    "data = {\n",
    "    'ID': np.arange(1, 101),\n",
    "    'Name': [f'User {i}' for i in range(1, 101)],\n",
    "    'Age': np.random.randint(18, 65, size=100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "df.to_parquet('dummy_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "210157ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>User 1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>User 2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>User 3</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>User 4</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>User 5</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>User 96</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>User 97</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>User 98</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>User 99</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>User 100</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID      Name  Age\n",
       "0     1    User 1   18\n",
       "1     2    User 2   23\n",
       "2     3    User 3   36\n",
       "3     4    User 4   49\n",
       "4     5    User 5   56\n",
       "..  ...       ...  ...\n",
       "95   96   User 96   26\n",
       "96   97   User 97   25\n",
       "97   98   User 98   46\n",
       "98   99   User 99   38\n",
       "99  100  User 100   51\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c11ad-11f8-4684-a0d5-94f30f054a49",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e3325-39a0-4f6f-aab1-ff9e983199d5",
   "metadata": {},
   "source": [
    "\n",
    "A pickle file is a serialized binary file format used in Python to store and exchange Python objects. It's named after the concept of \"pickling,\" which is a common term for serializing objects in Python. Pickling allows you to convert complex Python objects, such as lists, dictionaries, classes, and more, into a binary representation that can be easily saved to a file or transmitted over a network.\n",
    "\n",
    "Here are some key characteristics of pickle files:\n",
    "\n",
    "Serialization: Pickle files are used for the serialization of Python objects. Serialization is the process of converting an object's state into a binary format that can be stored or transmitted.\n",
    "\n",
    "Binary Format: Pickle files are binary files, meaning they are not human-readable. They contain a binary representation of the object's data and structure.\n",
    "\n",
    "Cross-Compatible: Pickle files are specific to Python, and they are generally not compatible with other programming languages. However, Python provides modules to serialize and deserialize objects in other formats like JSON, which are more widely compatible.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Data Persistence: You can use pickle files to save Python objects to disk, preserving their state and structure. This is commonly used for saving machine learning models, caching data, or storing program configurations.\n",
    "Interprocess Communication: Pickle files can be used to pass Python objects between different Python processes or scripts.\n",
    "Network Communication: Pickle files can be used to send Python objects over a network connection.\n",
    "Security Considerations: It's important to note that loading pickle files from untrusted or unauthenticated sources can pose security risks, as malicious code could be executed when unpickling objects. Therefore, it's generally recommended to be cautious when working with pickle files from untrusted sources.\n",
    "\n",
    "To work with pickle files in Python, you can use the pickle module, which provides functions like pickle.dump() to save objects to a file and pickle.load() to load objects from a file. Here's a simple example:\n",
    "\n",
    " it back into Python.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2afbbd9f-e666-4e8f-bb09-85483ee289e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John', 'age': 30}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "# Create a Python object\n",
    "data = {'name': 'John', 'age': 30}\n",
    "\n",
    "# Serialize and save it to a pickle file\n",
    "with open('data.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)\n",
    "\n",
    "# Load the object from the pickle file\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "print(loaded_data)  # Output: {'name': 'John', 'age': 30}\n",
    "#In this example, we create a dictionary, serialize it to a pickle file, and then load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272bee2-7050-47a3-afc2-a776622aa4fb",
   "metadata": {},
   "source": [
    "## Difference between Pickle and Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93203b-67e1-4aee-b5be-1b5255264d2f",
   "metadata": {},
   "source": [
    "\n",
    "Certainly! Here's a concise summary of the key differences between Pickle and Parquet:\n",
    "\n",
    "Pickle is a Python-specific binary format for serializing Python objects, preserving their state and structure. It's used for saving and loading Python objects within the Python ecosystem.\n",
    "\n",
    "Parquet is a cross-compatible binary file format optimized for efficient storage and analysis of structured tabular data, often used in big data processing frameworks. It's not Python-specific and is suitable for various programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebc6e3d7-7cf4-48ac-9b35-333f8ed26257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read_clipboard',\n",
       " 'read_csv',\n",
       " 'read_excel',\n",
       " 'read_feather',\n",
       " 'read_fwf',\n",
       " 'read_gbq',\n",
       " 'read_hdf',\n",
       " 'read_html',\n",
       " 'read_json',\n",
       " 'read_orc',\n",
       " 'read_parquet',\n",
       " 'read_pickle',\n",
       " 'read_sas',\n",
       " 'read_spss',\n",
       " 'read_sql',\n",
       " 'read_sql_query',\n",
       " 'read_sql_table',\n",
       " 'read_stata',\n",
       " 'read_table',\n",
       " 'read_xml']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile(r'read')\n",
    "list(filter(regex.match, dir(pd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f46b3-d02b-4d6e-ab43-21120e5aee1f",
   "metadata": {},
   "source": [
    "## SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbb9bb-82b7-4fcd-84e7-99d4ca7edfba",
   "metadata": {},
   "source": [
    "SAS files refer to datasets and other data-related files used by the SAS (Statistical Analysis System) software, a popular statistical and data analysis tool used in various fields such as statistics, data analytics, and business intelligence. SAS files encompass various types of data and metadata files used within the SAS ecosystem. Here are some common types of SAS files:\n",
    "\n",
    "1. **SAS Data Sets (.sas7bdat)**: These are binary files used to store structured data tables. SAS data sets contain data variables, observations, and metadata describing the structure of the data, including variable names, data types, and labels. These files are commonly used for data analysis and manipulation within SAS.\n",
    "\n",
    "2. **SAS Program Files (.sas)**: SAS program files are plain text files that contain SAS code written in the SAS programming language. SAS programs are used to perform data analysis, generate reports, and automate tasks within SAS.\n",
    "\n",
    "3. **SAS Catalogs (.sas7bcat)**: SAS catalogs store metadata information about SAS data sets, libraries, and other objects. They can include information about data set attributes, variable formats, and more. SAS catalogs are essential for managing and organizing SAS data.\n",
    "\n",
    "4. **SAS Output Files (.log, .lst, .out)**: SAS generates log, listing (lst), and output (out) files when executing SAS programs. The log file contains information about the execution of the SAS program, including error messages and warnings. The listing and output files often contain the results of data analysis and reports generated by SAS programs.\n",
    "\n",
    "5. **SAS Macro Files (.sas)**: SAS macros are reusable code segments or programs that can be invoked within SAS programs. SAS macro files contain macro definitions and are used to automate repetitive tasks and enhance code modularity.\n",
    "\n",
    "6. **SAS Transport Files (.xpt)**: SAS transport files are used for data exchange between different SAS environments or versions. They can store data sets, formats, and other metadata in a standardized format that is portable across different platforms.\n",
    "\n",
    "7. **SAS Access Database Files**: SAS can access various external data sources, including relational databases, Excel files, and more. SAS access files are used to connect to and access data from these external sources.\n",
    "\n",
    "8. **SAS Metadata Files (.sas7bmdp)**: These files store metadata information about SAS metadata repositories, which help manage and organize metadata for various SAS objects and resources.\n",
    "\n",
    "SAS files are an integral part of working with SAS software, allowing users to store, analyze, and manage data and code efficiently. SAS is commonly used in industries such as healthcare, finance, market research, and academia for its data analysis and statistical capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4de491-0d30-42e6-b89c-12bcec4b359b",
   "metadata": {},
   "source": [
    "## Create dummy SAS data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdf45f-6e5f-4aa9-8460-ccd16501f8dd",
   "metadata": {},
   "source": [
    "https://pypi.org/project/pyreadstat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4d78e4b-5db2-40fa-955d-73108ec53609",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/fd/f4/d3098b49c073da3e0ba2d6fb7c7fc7355729386331532422ed1d2ba57338/pyreadstat-1.2.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.2.3-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pyreadstat) (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyreadstat) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n",
      "Downloading pyreadstat-1.2.3-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.4 MB 919.0 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.4 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.3\n"
     ]
    }
   ],
   "source": [
    "! pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f4d46f1-ffca-43e2-b05c-27698cd64426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dummy dataset using pandas\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a SAS dataset manually as bytes\n",
    "sas_data = b\"\"\"LIBNAME mydata 'C:/path/to/sas7bdat/files';\n",
    "DATA mydata.dummy;\n",
    "    INPUT ID 3. Name $12.;\n",
    "    DATALINES4;\n",
    "\"\"\"\n",
    "\n",
    "# Add data rows\n",
    "for _, row in df.iterrows():\n",
    "    sas_data += f\"{row['ID']} '{row['Name']}';\\n\".encode()\n",
    "\n",
    "# Close the dataset\n",
    "sas_data += b\"\"\"\n",
    ";;;;\n",
    "RUN;\n",
    "\"\"\"\n",
    "\n",
    "# Save the SAS dataset to a .sas file\n",
    "with open('dummy.sas', 'wb') as sas_file:\n",
    "    sas_file.write(sas_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d6202-763c-48b1-becd-da87ad756451",
   "metadata": {},
   "source": [
    "## SPSS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1f2ac85-1017-4688-b934-7cfb1df862d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  Score\n",
      "0  1.0   85.0\n",
      "1  2.0   92.0\n",
      "2  3.0   78.0\n",
      "3  4.0   88.0\n",
      "4  5.0   95.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "# Create a dummy dataset using pandas\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Score': [85, 92, 78, 88, 95]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the SPSS file path\n",
    "spss_file_path = 'dummy.sav'\n",
    "\n",
    "# Save the DataFrame to the SPSS file\n",
    "pyreadstat.write_sav(df, spss_file_path)\n",
    "\n",
    "# Optional: Read and verify the contents of the SPSS file\n",
    "df_from_spss, meta = pyreadstat.read_sav(spss_file_path)\n",
    "print(df_from_spss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10efc07-abaf-457f-a2e2-d3999f666135",
   "metadata": {},
   "source": [
    "## .SAV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95d309-1f88-4341-b078-b30b13968bbd",
   "metadata": {},
   "source": [
    "The SPSS data file (.sav) was originally developed as the file format for the computer program IBM SPSS. Today, it is the most widely used format for storing survey data and is created by and analyzed by most advanced data analysis software.\n",
    "\n",
    "Overview of the file format\n",
    "SPSS data files, often called \"S A V\" files, are binary files. The key feature of the file format is that it is very rich in terms of metadata. In particular, it has a very rich amount of metadata stored for each variable, including:\n",
    "\n",
    "A variable Name.  E.g., Attitude.\n",
    "A Variable Label. E.g., Attitude may have the label How strongly do you agree with the statement â€˜Data Science is Coolâ€™?\n",
    "Each variable has a set of Value Labels. For example, a 1 for Gender may mean Male and a 2 may mean Female.\n",
    "One variable may be flagged as a weight and another as a filter.\n",
    "Related variables may be grouped into Multiple Response Sets.\n",
    "Certain values may be flagged as Missing Values.\n",
    "The scale type of variables will be stored as Nominal, Ordinal, or Scale.\n",
    "Information about the date format may be stored.\n",
    "Strengths of the file format\n",
    "The richness of the metadata makes the SPSS Data File a good format for storing survey data. This, combined with the file format being 50+ years old, has made it, by far, the most widely used file format for science survey-based data analysis (e.g., psychology, marketing, sociology, politics, market research, social research, polling).\n",
    "\n",
    "Weaknesses of the file format\n",
    "The file format has evolved over its 50+ years of existence, and this can cause some compatibility issues, particularly with text variables.\n",
    "Poor support for very long variable and value labels.\n",
    "The file format cannot be used for very large data files. This is because the file format requires the whole file to be read into memory in order to be analyzed.\n",
    "Limited support for metadata for variable sets. It only supports multiple response questions and not grids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339df82c-2eed-4e34-bfb2-a228f5c586dc",
   "metadata": {},
   "source": [
    "## SQL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f73509d-f0e4-44da-a594-e7df24a5c4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 22, 35, 28]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac3e5afa-82ea-4059-aa9a-0f3411ea82d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>David</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Eve</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Name  Age\n",
       "0   1    Alice   25\n",
       "1   2      Bob   30\n",
       "2   3  Charlie   22\n",
       "3   4    David   35\n",
       "4   5      Eve   28"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5542302c-0a81-4ad8-8a2f-47ecaf46ad7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x245539d0140>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect('dummy.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the SQL command to create a table\n",
    "create_table_query = '''\n",
    "CREATE TABLE IF NOT EXISTS dummy_data (\n",
    "    ID INT PRIMARY KEY,\n",
    "    Name TEXT,\n",
    "    Age INT\n",
    ");\n",
    "'''\n",
    "\n",
    "# Execute the SQL command\n",
    "cursor.execute(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f791d42-bd83-4a9a-86de-f7924f39161f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eda5161a-347a-4fa8-8247-52adf6e6f9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x24556018140>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconnect to the SQLite database\n",
    "conn = sqlite3.connect('dummy.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Convert the DataFrame to a list of tuples\n",
    "data_to_insert = [tuple(row) for row in df.values]\n",
    "\n",
    "# Define the SQL command to insert data\n",
    "insert_data_query = 'INSERT INTO dummy_data (ID, Name, Age) VALUES (?, ?, ?);'\n",
    "\n",
    "# Execute the SQL command for each row of data\n",
    "cursor.executemany(insert_data_query, data_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41837f07-986f-461d-9681-c1a1bee1ee5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33571c78-3282-49c0-835f-bfdf3ce02ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice', 25)\n",
      "(2, 'Bob', 30)\n",
      "(3, 'Charlie', 22)\n",
      "(4, 'David', 35)\n",
      "(5, 'Eve', 28)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('dummy.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the SELECT query\n",
    "select_query = 'SELECT * FROM dummy_data;'\n",
    "\n",
    "# Execute the SELECT query\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Display the results\n",
    "for row in results:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229d75c7-63cc-47c3-91a9-c8ad75142403",
   "metadata": {},
   "source": [
    "https://www.postgresql.org/download/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe67c36-97e7-4ed2-bcba-2864764e9324",
   "metadata": {},
   "source": [
    "## STATA Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a97d4-f815-48c7-8ad0-ccd8b742cf40",
   "metadata": {},
   "source": [
    "Stata files are data files associated with Stata, a popular statistical software package used for data analysis, manipulation, and visualization. Stata files are used to store structured datasets, and they come in different formats, each serving a specific purpose:\n",
    "\n",
    "1. **Stata Data File (.dta)**: This is the primary data file format used by Stata. It stores data in a binary format, preserving variable names, labels, data types, and other metadata. Stata data files can store both data and command results.\n",
    "\n",
    "2. **Stata Program Files (.do)**: These are plain text files that contain Stata programming code written in the Stata scripting language. Stata program files are used to automate tasks, perform data analysis, and generate reports. They can be executed within Stata's interactive environment.\n",
    "\n",
    "3. **Stata Log Files (.log)**: Stata log files capture the entire session of commands and output when running Stata programs or interacting with data. They are useful for documenting and replicating analyses.\n",
    "\n",
    "4. **Stata Output Files (.out)**: These files store output generated by Stata when running programs or commands. They often contain statistical summaries, regression results, and other analysis outputs.\n",
    "\n",
    "5. **Stata Dictionary Files (.ado)**: These files contain user-written Stata commands or \"programs.\" Users can create custom commands or functions in Stata using .ado files, which can then be invoked like built-in Stata commands.\n",
    "\n",
    "6. **Stata Journal Files (.sj)**: These files are used for the Stata Journal, a publication featuring articles on using Stata for data analysis. They typically contain articles, code, and datasets used in the journal.\n",
    "\n",
    "7. **Stata Graphics Files (.gph)**: These files store graphs and plots generated by Stata. Stata provides a wide range of graphing options for data visualization, and the results can be saved in .gph files.\n",
    "\n",
    "8. **Stata Viewer Files (.stview)**: Stata Viewer is a separate program for viewing Stata data and results files. .stview files are associated with Stata Viewer and are used to store data and results for viewing and sharing with others.\n",
    "\n",
    "Stata files are widely used in fields such as economics, social sciences, epidemiology, and public health for data analysis and statistical research. The .dta format, in particular, is the standard for storing datasets in Stata, making it easy to share and collaborate on data-driven projects within the Stata ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6393d096-972e-4e8f-912a-571f28deb4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dummy dataset using pandas\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 22, 35, 28]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a Stata file using pandas\n",
    "df.to_stata('dummy.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c8f64-5879-4d45-9dd6-5b4cd647b021",
   "metadata": {},
   "source": [
    "## Read Table files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025294b7-16a9-4209-98cd-d32f2dc93496",
   "metadata": {},
   "source": [
    "read_table is a function used in some libraries like pandas in Python for reading tabular data from various sources and creating a DataFrame. The read_table function is typically used to read data from text files where columns are separated by a delimiter, often a tab character ('\\t') or other specified characters.\n",
    "\n",
    "In pandas, read_table has been deprecated in favor of more versatile functions like read_csv and read_csv. These functions are more commonly used for reading tabular data because they provide more options and better support for various file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00135970-2581-443a-a506-14f7dc7082ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d62316-3222-4eb5-97f4-a536242fcabc",
   "metadata": {},
   "source": [
    "XML (Extensible Markup Language) files are a popular format for storing and exchanging structured data. XML is a markup language that uses tags to define elements and their hierarchical relationships within a document. XML files are both human-readable and machine-readable, making them suitable for a wide range of applications, including data storage, configuration files, and data interchange between different systems.\n",
    "\n",
    "Key characteristics of XML files include:\n",
    "\n",
    "1. **Hierarchical Structure**: XML documents are organized hierarchically, consisting of elements nested within other elements. Elements can have attributes and contain text data, other elements, or a combination of both.\n",
    "\n",
    "2. **Tags**: XML uses tags to define elements. Tags are enclosed in angle brackets (< >) and come in pairs: an opening tag and a closing tag. For example, `<book>` is an opening tag, and `</book>` is a closing tag.\n",
    "\n",
    "3. **Attributes**: Elements can have attributes that provide additional information about the element. Attributes are typically name-value pairs and are specified within the opening tag. For example, `<book title=\"Introduction to XML\">`.\n",
    "\n",
    "4. **Nesting**: Elements can be nested within other elements to represent a hierarchical structure. For example:\n",
    "\n",
    "   ```xml\n",
    "   <library>\n",
    "       <book title=\"Introduction to XML\">\n",
    "           <author>John Doe</author>\n",
    "           <price>29.99</price>\n",
    "       </book>\n",
    "       <book title=\"Data Science Basics\">\n",
    "           <author>Jane Smith</author>\n",
    "           <price>34.95</price>\n",
    "       </book>\n",
    "   </library>\n",
    "   ```\n",
    "\n",
    "5. **Human-Readable**: XML files are plain text and can be easily read and edited by humans using a text editor. The hierarchical structure and use of tags make the data's organization clear.\n",
    "\n",
    "6. **Machine-Readable**: XML files can be parsed and processed by software applications, making them suitable for data exchange between different systems.\n",
    "\n",
    "XML is widely used in various domains, including web services (SOAP and REST), configuration files (e.g., XML configuration files for software applications), data interchange formats, and data representation in databases. Additionally, XML has served as the foundation for other markup languages like XHTML and is closely related to HTML (Hypertext Markup Language) used for web page structure.\n",
    "\n",
    "XML files are identified by the '.xml' file extension, and they adhere to well-defined rules and standards outlined in the XML specification, making them a versatile choice for data representation and exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce525740-784e-4d4f-ace6-e898c6295c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Create the root element\n",
    "root = ET.Element(\"library\")\n",
    "\n",
    "# Create child elements\n",
    "book1 = ET.SubElement(root, \"book\")\n",
    "book1_title = ET.SubElement(book1, \"title\")\n",
    "book1_title.text = \"Introduction to XML\"\n",
    "book1_author = ET.SubElement(book1, \"author\")\n",
    "book1_author.text = \"John Doe\"\n",
    "book1_price = ET.SubElement(book1, \"price\")\n",
    "book1_price.text = \"29.99\"\n",
    "\n",
    "book2 = ET.SubElement(root, \"book\")\n",
    "book2_title = ET.SubElement(book2, \"title\")\n",
    "book2_title.text = \"Data Science Basics\"\n",
    "book2_author = ET.SubElement(book2, \"author\")\n",
    "book2_author.text = \"Jane Smith\"\n",
    "book2_price = ET.SubElement(book2, \"price\")\n",
    "book2_price.text = \"34.95\"\n",
    "\n",
    "# Create an ElementTree from the root element\n",
    "tree = ET.ElementTree(root)\n",
    "\n",
    "# Save the XML to a file\n",
    "tree.write(\"dummy.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd435ef6-610f-4379-aa35-dc6aa36d83fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xml.etree.ElementTree.ElementTree at 0x24556473010>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "941766d7-a044-4e44-bb76-fcab5bae57fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_xml('dummy.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f45abf12-67bf-4cfe-a2b6-a5de4906d8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introduction to XML</td>\n",
       "      <td>John Doe</td>\n",
       "      <td>29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Basics</td>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>34.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title      author  price\n",
       "0  Introduction to XML    John Doe  29.99\n",
       "1  Data Science Basics  Jane Smith  34.95"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595e7ad-1b39-4638-a4b4-22960a5f7dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
